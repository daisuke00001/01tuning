#!/usr/bin/env python3
"""
patent_idã®å•é¡Œã‚’èª¿æŸ»ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
from pathlib import Path
from collections import Counter

def check_patent_ids():
    """patent_idã®çŠ¶æ³ã‚’è©³ã—ãèª¿æŸ»"""
    
    print("ğŸ” Patent IDå•é¡Œã®èª¿æŸ»")
    print("=" * 50)
    
    # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
    data_path = Path("data/cleaned/cleaned_patents_medium.json")
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
    
    # patent_idã®åˆ†å¸ƒã‚’ç¢ºèª
    patent_ids = []
    for item in data:
        pid = item.get('patent_id', 'MISSING_KEY')
        if pid is None:
            pid = 'NULL_VALUE'
        elif pid == '':
            pid = 'EMPTY_STRING'
        patent_ids.append(pid)
    
    # é »åº¦ã‚’ç¢ºèª
    id_counter = Counter(patent_ids)
    print(f"\nğŸ“‹ Patent IDåˆ†å¸ƒ:")
    for pid, count in id_counter.most_common(10):
        print(f"  '{pid}': {count}ä»¶")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ç¢ºèª
    print(f"\nğŸ” æœ€åˆã®5ä»¶ã®è©³ç´°:")
    for i, item in enumerate(data[:5]):
        print(f"\n{i+1}ä»¶ç›®:")
        print(f"  patent_id: '{item.get('patent_id', 'MISSING')}'")
        print(f"  section: '{item.get('section', 'MISSING')}'")
        print(f"  text: '{item.get('text', 'MISSING')[:50]}...'")
    
    # å•é¡Œã®åˆ†æ
    unique_ids = set(patent_ids)
    print(f"\nğŸ“ˆ çµ±è¨ˆ:")
    print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯patent_idæ•°: {len(unique_ids)}")
    print(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
    print(f"  - å¹³å‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°/ç‰¹è¨±: {len(data) / len(unique_ids):.1f}")
    
    if len(unique_ids) == 1:
        print(f"  âš ï¸ å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒåŒã˜patent_idã‚’æŒã£ã¦ã„ã¾ã™: '{list(unique_ids)[0]}'")
        return False
    elif len(unique_ids) < 10:
        print(f"  âš ï¸ patent_idã®ç¨®é¡ãŒå°‘ãªã™ãã¾ã™")
        return False
    else:
        print(f"  âœ… patent_IDã¯æ­£å¸¸ã«åˆ†æ•£ã—ã¦ã„ã¾ã™")
        return True

def investigate_original_data():
    """å…ƒã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰ãƒ‡ãƒ¼ã‚¿ã‚‚ç¢ºèª"""
    
    print(f"\n" + "=" * 50)
    print("ğŸ” å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆsections_datasetï¼‰ã®èª¿æŸ»")
    
    sections_path = Path("data/cleaned/cleaned_sections_dataset.json")
    if not sections_path.exists():
        print("âŒ sections_datasetãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    with open(sections_path, 'r', encoding='utf-8') as f:
        sections_data = json.load(f)
    
    print(f"ğŸ“Š Sectionsãƒ‡ãƒ¼ã‚¿æ•°: {len(sections_data)}")
    
    # patent_idã®åˆ†å¸ƒ
    section_patent_ids = [item.get('patent_id', 'MISSING') for item in sections_data]
    section_id_counter = Counter(section_patent_ids)
    
    print(f"ğŸ“‹ Sections Patent IDåˆ†å¸ƒ (ä¸Šä½10ä»¶):")
    for pid, count in section_id_counter.most_common(10):
        print(f"  '{pid}': {count}ä»¶")
    
    # æ¯”è¼ƒ
    unique_section_ids = set(section_patent_ids)
    print(f"\nğŸ“ˆ Sectionsçµ±è¨ˆ:")
    print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯patent_idæ•°: {len(unique_section_ids)}")
    print(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(sections_data)}")
    print(f"  - å¹³å‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°/ç‰¹è¨±: {len(sections_data) / len(unique_section_ids):.1f}")

if __name__ == "__main__":
    # patent_idå•é¡Œã®èª¿æŸ»
    is_normal = check_patent_ids()
    
    # å…ƒãƒ‡ãƒ¼ã‚¿ã®èª¿æŸ»
    investigate_original_data()
    
    if not is_normal:
        print(f"\nğŸš¨ æ ¹æœ¬åŸå› :")
        print(f"  XMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®æŠ½å‡ºæ™‚ã«patent_idãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ãªã„")
        print(f"  â†’ src/patent_processing/text_processor.py ã®ä¿®æ­£ãŒå¿…è¦")