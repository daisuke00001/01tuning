{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkOWLY8G6c+PZECx6ChPbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisuke00001/01tuning/blob/main/notebooks/TinySwallow_Patent_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TinySwallowç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å°‚ç”¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n"
      ],
      "metadata": {
        "id": "ySSArs5y8K85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç’°å¢ƒã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
      ],
      "metadata": {
        "id": "hkArpSia-djD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxHWyX1h0Dq9"
      },
      "outputs": [],
      "source": [
        "# å®Ÿè¡Œç’°å¢ƒã®ç¢ºèªã¨githubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³\n",
        "import os\n",
        "import subprocess\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. è‡ªå‹•ãƒªãƒ­ãƒ¼ãƒ‰è¨­å®šï¼ˆåˆå›ã®ã¿ï¼‰\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# 2. æ›´æ–°ãŒã‚ã‚‹ãŸã³ã«å®Ÿè¡Œ\n",
        "!git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjTKHUaO_Wz",
        "outputId": "560f3e4b-ae19-4149-cbf9-2657fb552af5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (5/5), 1.11 KiB | 1.11 MiB/s, done.\n",
            "From https://github.com/daisuke00001/01tuning\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   00404b7..0f6cac9  main       -> origin/main\n",
            "Updating 00404b7..0f6cac9\n",
            "Fast-forward\n",
            " src/config.py      | 19 \u001b[32m+++++++++++++++++++\u001b[m\n",
            " src/model_utils.py | 26 \u001b[32m+++++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 44 insertions(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUã®ç¢ºèª\n",
        "def setup_patent_environment():\n",
        "    \"\"\"ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å­¦åå®¹Google Colabç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\"\"\"\n",
        "    print(\"ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...\")\n",
        "\n",
        "    # GPUç¢ºèª\n",
        "    if not os.path.exists('/opt/bin/nvidia-smi'):\n",
        "       print(\"GPUç’°å¢ƒãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã€‚ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¿ã‚¤ãƒ—ã‚’GPUã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚\")\n",
        "       return False\n",
        "\n",
        "    # ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³\n",
        "    repo_url = \"https://github.com/daisuke00001/01tuning.git\"\n",
        "    if os.path.exists(\"01tuning\"):\n",
        "        print(\"ğŸ“ ãƒªãƒã‚¸ãƒˆãƒªãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚æœ€æ–°ç‰ˆã«æ›´æ–°ä¸­...\")\n",
        "        subprocess.run([\"git\", \"-C\", \"01tuning\", \"pull\"], check=True)\n",
        "    else:\n",
        "        print(f\"ğŸ“ ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­: {repo_url}\")\n",
        "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•\n",
        "    os.chdir(\"01tuning\")\n",
        "\n",
        "    # Pythonãƒ‘ã‚¹ã«è¿½åŠ \n",
        "    if \"/content/01tuning/src\" not in sys.path:\n",
        "        sys.path.append(\"/content/01tuning/src\")\n",
        "\n",
        "    print(\"âœ… ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")\n",
        "    return True\n",
        "\n",
        "# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Ÿè¡Œ\n",
        "setup_patent_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq3IroC01_5B",
        "outputId": "5fd7dce8-4335-47d9-cf1c-6771c24eab0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...\n",
            "ğŸ“ ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­: https://github.com/daisuke00001/01tuning.git\n",
            "âœ… ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ],
      "metadata": {
        "id": "zqSIn8XmJQUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colabå°‚ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "%%capture\n",
        "# Unslothã¨ãã®ä¾å­˜é–¢ä¿‚\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ©ãƒ¼å¯¾å¿œ\n",
        "!pip install xformers\n",
        "!pip uninstall torchvision -y\n",
        "!pip install torchvision --no-cache-dir\n",
        "\n",
        "# ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å‡¦ç†å°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "!pip install lxml beautifulsoup4 xmltodict\n",
        "!pip install nltk rouge-score sacrebleu\n",
        "!pip install openpyxl\n",
        "\n",
        "# ãã®ä»–ã®ä¾å­˜é–¢ä¿‚\n",
        "!pip install PyYAML\n",
        "\n",
        "print(\"ğŸ“¦ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
      ],
      "metadata": {
        "id": "QK932-njJOEC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç‰¹è¨±å°‚ç”¨è¨­å®šã®èª­ã¿è¾¼ã¿"
      ],
      "metadata": {
        "id": "DyOQcXRKJdWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from src.config import Config\n",
        "\n",
        "config_path = \"configs/patent_config.yaml\"\n",
        "config = Config.load_from_yaml(config_path)\n",
        "\n",
        "print(\"âš™ï¸ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨è¨­å®šã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ğŸ“„ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {config_path}\")\n",
        "print(f\"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {config.model.name}\")\n",
        "print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {config.dataset.name}\")\n",
        "print(f\"ğŸ”§ æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {config.model.max_seq_length}\")\n",
        "print(f\"ğŸ“ˆ æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {config.training.max_steps}\")\n",
        "print(f\"âš¡ ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"ğŸ¯ å­¦ç¿’ç‡: {config.training.learning_rate}\")\n",
        "print(f\"ğŸ”— LoRAãƒ©ãƒ³ã‚¯: {config.lora.r}\")\n",
        "print(f\"ğŸ“‹ å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {', '.join(config.lora.target_modules[:3])}...\")\n",
        "\n",
        "print(\"\\nğŸ­ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å›ºæœ‰è¨­å®š:\")\n",
        "print(f\"ğŸ“„ ç‰¹è¨±æ–‡æ›¸æœ€å¤§é•·: {config.dataset.max_patent_length}\")\n",
        "print(f\"ğŸ“ å®Ÿæ–½å½¢æ…‹æœ€å¤§é•·: {config.dataset.max_implementation_length}\")\n",
        "print(f\"ğŸ§¹ å‚ç…§é™¤å»: {config.dataset.remove_references}\")\n",
        "print(f\"âœ¨ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ•´ç†: {config.dataset.clean_formatting}\")\n",
        "print(f\"ğŸ“Š è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {', '.join(config.evaluation.metrics)}\")\n",
        "\n",
        "print(\"\\nâœ… ç‰¹è¨±å°‚ç”¨è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRjZ43tyJfzV",
        "outputId": "2fcc9ca5-f47e-4312-d869-471d2d4e8f5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨è¨­å®šã‚’èª­ã¿è¾¼ã¿ä¸­...\n",
            "==================================================\n",
            "ğŸ“„ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: configs/patent_config.yaml\n",
            "ğŸ¤– ãƒ¢ãƒ‡ãƒ«: SakanaAI/TinySwallow-1.5B-Instruct\n",
            "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: patent_japanese\n",
            "ğŸ”§ æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 2048\n",
            "ğŸ“ˆ æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: 200\n",
            "âš¡ ãƒãƒƒãƒã‚µã‚¤ã‚º: 1\n",
            "ğŸ¯ å­¦ç¿’ç‡: 3e-05\n",
            "ğŸ”— LoRAãƒ©ãƒ³ã‚¯: 16\n",
            "ğŸ“‹ å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: q_proj, v_proj, k_proj...\n",
            "\n",
            "ğŸ­ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å›ºæœ‰è¨­å®š:\n",
            "ğŸ“„ ç‰¹è¨±æ–‡æ›¸æœ€å¤§é•·: 2048\n",
            "ğŸ“ å®Ÿæ–½å½¢æ…‹æœ€å¤§é•·: 1024\n",
            "ğŸ§¹ å‚ç…§é™¤å»: True\n",
            "âœ¨ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ•´ç†: True\n",
            "ğŸ“Š è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: rouge, bleu, patent_implementation_quality\n",
            "\n",
            "âœ… ç‰¹è¨±å°‚ç”¨è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨LoRAè¨­å®š"
      ],
      "metadata": {
        "id": "FV5HnVQqJzYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model_utils import ModelManager\n",
        "import torch\n",
        "\n",
        "print(\"TinySwallow-1.5B ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
        "model_manager = ModelManager(config)\n",
        "\n",
        "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡(èª­ã¿è¾¼ã¿å‰)\n",
        "initial_memory = model_manager.get_memory_stats()\n",
        "print(f\"ğŸ’¾ æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory['used']:.2f}GB / {initial_memory['total']:.2f}GB ({initial_memory['percentage']:.1f}%)\")\n",
        "\n",
        "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
        "model, tokenizer = model_manager.load_model()\n",
        "\n",
        "# LoRAè¨­å®šï¼ˆç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ï¼‰\n",
        "print(\"\\nğŸ”§ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨LoRAè¨­å®šã‚’é©ç”¨ä¸­...\")\n",
        "model = model_manager.setup_lora()\n",
        "\n",
        "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆèª­ã¿è¾¼ã¿å¾Œï¼‰\n",
        "loaded_memory = model_manager.get_memory_stats()\n",
        "print(f\"ğŸ’¾ æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {loaded_memory['used']:.2f}GB / {loaded_memory['total']:.2f}GB ({loaded_memory['percentage']:.1f}%)\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º\n",
        "model_info = model_manager.get_model_info()  # âœ… æ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰\n",
        "print(f\"\\nğŸ” ãƒ¢ãƒ‡ãƒ«è©³ç´°æƒ…å ±:\")\n",
        "print(f\"ğŸ“Š ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['total_params']:,}\")\n",
        "print(f\"ğŸ”§ å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['trainable_params']:,}\")\n",
        "print(f\"ğŸ“ˆ å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰²åˆ: {model_info['trainable_percentage']:.2f}%\")\n",
        "\n",
        "print(\"\\nâœ… TinySwallow-1.5B + ç‰¹è¨±å°‚ç”¨LoRAè¨­å®šå®Œäº†\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpTe2MjkJ202",
        "outputId": "0b6aff2f-9a13-49cc-e54a-76feaa9f0249"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinySwallow-1.5B ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\n",
            "==================================================\n",
            "ğŸ’¾ æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 1.57GB / 14.74GB (10.6%)\n",
            "==((====))==  Unsloth 2025.7.11: Fast Qwen2 patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "ğŸ”§ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨LoRAè¨­å®šã‚’é©ç”¨ä¸­...\n",
            "ğŸ’¾ æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 2.91GB / 14.74GB (19.7%)\n",
            "\n",
            "ğŸ” ãƒ¢ãƒ‡ãƒ«è©³ç´°æƒ…å ±:\n",
            "ğŸ“Š ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 907,081,216\n",
            "ğŸ”§ å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 18,464,768\n",
            "ğŸ“ˆ å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰²åˆ: 2.04%\n",
            "\n",
            "âœ… TinySwallow-1.5B + ç‰¹è¨±å°‚ç”¨LoRAè¨­å®šå®Œäº†\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"
      ],
      "metadata": {
        "id": "2f2WEcQeLAKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Google Driveé€£æº\n",
        "print(\"ğŸ’¾ Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Google Driveãƒã‚¦ãƒ³ãƒˆæˆåŠŸ\")\n",
        "\n",
        "# Google Driveå†…ã®å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹\n",
        "drive_processed_dir = \"/content/drive/MyDrive/0731PatentData/processed\"\n",
        "local_processed_dir = \"/content/01tuning/data/processed\"\n",
        "\n",
        "# ãƒ­ãƒ¼ã‚«ãƒ«ã®processedãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
        "os.makedirs(local_processed_dir, exist_ok=True)\n",
        "\n",
        "if os.path.exists(drive_processed_dir):\n",
        "    print(f\"ğŸ“ Google Driveå†…ã®å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªä¸­...\")\n",
        "\n",
        "    # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯\n",
        "    available_files = []\n",
        "    target_files = [\n",
        "        \"chatml_training.json\",\n",
        "        \"complete_dataset.json\",\n",
        "        \"training_dataset.json\",\n",
        "        \"dataset_stats.json\"\n",
        "    ]\n",
        "\n",
        "    for filename in target_files:\n",
        "        drive_file_path = os.path.join(drive_processed_dir, filename)\n",
        "        if os.path.exists(drive_file_path):\n",
        "            available_files.append(filename)\n",
        "            print(f\"  âœ… {filename} ({os.path.getsize(drive_file_path)/1024/1024:.1f}MB)\")\n",
        "\n",
        "    if available_files:\n",
        "        # ChatMLå½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨\n",
        "        if \"chatml_training.json\" in available_files:\n",
        "            target_file = \"chatml_training.json\"\n",
        "        else:\n",
        "            target_file = available_files[0]\n",
        "\n",
        "        print(f\"\\nğŸ”„ {target_file} ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼ä¸­...\")\n",
        "        drive_file_path = os.path.join(drive_processed_dir, target_file)\n",
        "        local_file_path = os.path.join(local_processed_dir, target_file)\n",
        "\n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼\n",
        "        shutil.copy2(drive_file_path, local_file_path)\n",
        "        print(f\"âœ… ã‚³ãƒ”ãƒ¼å®Œäº†: {local_file_path}\")\n",
        "\n",
        "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆå½¢å¼ã«å¿œã˜ã¦åˆ†å²ï¼‰\n",
        "        print(f\"\\nğŸ“‚ {target_file} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "\n",
        "        try:\n",
        "            with open(local_file_path, 'r', encoding='utf-8') as f:\n",
        "                # ã¾ãšå…¨ä½“ã‚’JSONã¨ã—ã¦èª­ã¿è¾¼ã¿\n",
        "                patent_data = json.load(f)\n",
        "\n",
        "                # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
        "                if not isinstance(patent_data, list):\n",
        "                    patent_data = [patent_data]\n",
        "\n",
        "                print(f\"ğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆJSONé…åˆ—å½¢å¼ï¼‰:\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # JSONé…åˆ—å½¢å¼ã§å¤±æ•—ã—ãŸå ´åˆã€JSONLå½¢å¼ã‚’è©¦è¡Œ\n",
        "            print(\"ğŸ”„ JSONLå½¢å¼ã¨ã—ã¦å†è©¦è¡Œ...\")\n",
        "            with open(local_file_path, 'r', encoding='utf-8') as f:\n",
        "                patent_data = []\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    line = line.strip()\n",
        "                    if line:  # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—\n",
        "                        try:\n",
        "                            patent_data.append(json.loads(line))\n",
        "                        except json.JSONDecodeError as e:\n",
        "                            print(f\"âš ï¸ è¡Œ {line_num} ã§JSONã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "                            continue\n",
        "\n",
        "                print(f\"ğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆJSONLå½¢å¼ï¼‰:\")\n",
        "\n",
        "        print(f\"  ğŸ“„ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(patent_data):,}\")\n",
        "\n",
        "        if len(patent_data) > 0:\n",
        "            # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®æ§‹é€ ã‚’ç¢ºèª\n",
        "            sample = patent_data[0]\n",
        "            print(f\"  ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {list(sample.keys())}\")\n",
        "\n",
        "            # ChatMLå½¢å¼ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆçµ±è¨ˆã‚’è¨ˆç®—\n",
        "            if 'messages' in sample:\n",
        "                print(f\"  ğŸ¯ ãƒ‡ãƒ¼ã‚¿å½¢å¼: ChatML (messageså½¢å¼)\")\n",
        "                # messagesã‚’å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’è¨ˆç®—\n",
        "                total_chars = 0\n",
        "                for item in patent_data[:100]:  # æœ€åˆã®100ä»¶ã§ã‚µãƒ³ãƒ—ãƒ«è¨ˆç®—\n",
        "                    if 'messages' in item:\n",
        "                        text_length = sum(len(msg.get('content', '')) for msg in item['messages'])\n",
        "                        total_chars += text_length\n",
        "                avg_length = total_chars / min(len(patent_data), 100)\n",
        "                print(f\"  ğŸ“ å¹³å‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·: {avg_length:.0f} æ–‡å­—\")\n",
        "\n",
        "            elif 'text' in sample:\n",
        "                print(f\"  ğŸ¯ ãƒ‡ãƒ¼ã‚¿å½¢å¼: å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼\")\n",
        "                avg_length = sum(len(item.get('text', '')) for item in patent_data[:100]) / min(len(patent_data), 100)\n",
        "                print(f\"  ğŸ“ å¹³å‡ãƒ†ã‚­ã‚¹ãƒˆé•·: {avg_length:.0f} æ–‡å­—\")\n",
        "\n",
        "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆæƒ…å ±ã‚‚èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
        "        stats_file = \"dataset_stats.json\"\n",
        "        if stats_file in available_files:\n",
        "            stats_drive_path = os.path.join(drive_processed_dir, stats_file)\n",
        "            stats_local_path = os.path.join(local_processed_dir, stats_file)\n",
        "            shutil.copy2(stats_drive_path, stats_local_path)\n",
        "\n",
        "            with open(stats_local_path, 'r', encoding='utf-8') as f:\n",
        "                stats = json.load(f)\n",
        "\n",
        "        # æ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å®‰å…¨ãªé©ç”¨\n",
        "        total_patents = stats.get('total_patents', 'N/A')\n",
        "        total_claims = stats.get('total_claims', 'N/A')\n",
        "        total_sentences = stats.get('total_sentences', 'N/A')\n",
        "\n",
        "        print(f\"  ğŸ›ï¸ ç‰¹è¨±æ•°: {total_patents:,}\" if isinstance(total_patents, (int, float)) else f\"  ğŸ›ï¸ ç‰¹è¨±æ•°: {total_patents}\")\n",
        "        print(f\"  ğŸ“‹ è«‹æ±‚é …æ•°: {total_claims:,}\" if isinstance(total_claims, (int, float)) else f\"  ğŸ“‹ è«‹æ±‚é …æ•°: {total_claims}\")\n",
        "        print(f\"  ğŸ“„ ç·æ–‡æ•°: {total_sentences:,}\" if isinstance(total_sentences, (int, float)) else f\"  ğŸ“„ ç·æ–‡æ•°: {total_sentences}\")\n",
        "\n",
        "        # Hugging Face Datasetå½¢å¼ã«å¤‰æ›\n",
        "        dataset = Dataset.from_list(patent_data)\n",
        "        print(f\"\\nğŸ¯ Hugging Face Datasetå½¢å¼ã«å¤‰æ›å®Œäº†\")\n",
        "        print(f\"  Dataset({len(dataset)} rows)\")\n",
        "\n",
        "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
        "        print(f\"\\nğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®1ä»¶ï¼‰:\")\n",
        "        if len(dataset) > 0:\n",
        "            sample = dataset[0]\n",
        "            if 'messages' in sample:\n",
        "                print(f\"  å½¢å¼: ChatML (messages: {len(sample['messages'])} ä»¶)\")\n",
        "                for i, msg in enumerate(sample['messages'][:2]):  # æœ€åˆã®2ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º\n",
        "                    content_preview = msg.get('content', '')[:200] + \"...\" if len(msg.get('content', '')) > 200 else msg.get('content', '')\n",
        "                    print(f\"    {i+1}. {msg.get('role', 'unknown')}: {content_preview}\")\n",
        "            elif 'text' in sample:\n",
        "                text_preview = sample['text'][:300] + \"...\" if len(sample['text']) > 300 else sample['text']\n",
        "                print(f\"  ãƒ†ã‚­ã‚¹ãƒˆ: {text_preview}\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ åˆ©ç”¨å¯èƒ½ãªå‰å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
        "        dataset = None\n",
        "else:\n",
        "    print(f\"âŒ Google Driveå†…ã«å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {drive_processed_dir}\")\n",
        "    print(\"ğŸ“ ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:\")\n",
        "    print(\"   Google Drive/PatentData/processed/\")\n",
        "    print(\"     â”œâ”€â”€ chatml_training.json\")\n",
        "    print(\"     â”œâ”€â”€ complete_dataset.json\")\n",
        "    print(\"     â””â”€â”€ dataset_stats.json\")\n",
        "    dataset = None\n",
        "\n",
        "if dataset is not None:\n",
        "    print(\"\\nâœ… ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†ï¼\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Google Driveã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShdiSWW8LDms",
        "outputId": "784b6249-20cb-40b2-8e22-18e88e2c730e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...\n",
            "==================================================\n",
            "ğŸ’¾ Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Driveãƒã‚¦ãƒ³ãƒˆæˆåŠŸ\n",
            "ğŸ“ Google Driveå†…ã®å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªä¸­...\n",
            "  âœ… chatml_training.json (45.1MB)\n",
            "  âœ… complete_dataset.json (123.4MB)\n",
            "  âœ… training_dataset.json (38.9MB)\n",
            "  âœ… dataset_stats.json (0.0MB)\n",
            "\n",
            "ğŸ”„ chatml_training.json ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼ä¸­...\n",
            "âœ… ã‚³ãƒ”ãƒ¼å®Œäº†: /content/01tuning/data/processed/chatml_training.json\n",
            "\n",
            "ğŸ“‚ chatml_training.json ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\n",
            "ğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆJSONé…åˆ—å½¢å¼ï¼‰:\n",
            "  ğŸ“„ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: 420\n",
            "  ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ§‹é€ : ['messages', 'metadata']\n",
            "  ğŸ¯ ãƒ‡ãƒ¼ã‚¿å½¢å¼: ChatML (messageså½¢å¼)\n",
            "  ğŸ“ å¹³å‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·: 60278 æ–‡å­—\n",
            "  ğŸ›ï¸ ç‰¹è¨±æ•°: N/A\n",
            "  ğŸ“‹ è«‹æ±‚é …æ•°: N/A\n",
            "  ğŸ“„ ç·æ–‡æ•°: N/A\n",
            "\n",
            "ğŸ¯ Hugging Face Datasetå½¢å¼ã«å¤‰æ›å®Œäº†\n",
            "  Dataset(420 rows)\n",
            "\n",
            "ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®1ä»¶ï¼‰:\n",
            "  å½¢å¼: ChatML (messages: 3 ä»¶)\n",
            "    1. system: ã‚ãªãŸã¯ç‰¹è¨±æ–‡æ›¸ã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã«åŸºã¥ã„ã¦ã€ãã®ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªå½¢æ…‹ã‚’è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
            "    2. user: ä»¥ä¸‹ã®ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã«åŸºã¥ã„ã¦ã€ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ï¼š\n",
            "\n",
            "ã€è«‹æ±‚é …1ã€‘\n",
            "ç²¾ç¥éšœå®³ã§ã‚ã‚‹ã¨è¨ºæ–­ã•ã‚ŒãŸæ‚£è€…ã‚’å«ã‚€å­¦ç¿’ç”¨å‹•ç”»ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å­¦ç¿’ã•ã›ãŸç²¾ç¥éšœå®³å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è¨˜æ†¶ã•ã›ã‚‹è¨˜æ†¶éƒ¨ã¨ã€ åˆ†æå¯¾è±¡è€…ã®æ—¥å¸¸ä¼šè©±ã«ä¿‚ã‚‹åˆ†æå¯¾è±¡å‹•ç”»ã‚’å–å¾—ã™ã‚‹å–å¾—éƒ¨ã¨ã€ å‰è¨˜åˆ†æå¯¾è±¡å‹•ç”»ã«å‰è¨˜ç²¾ç¥éšœå®³å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã—ã¦ã€å‰è¨˜åˆ†æå¯¾è±¡è€…ãŒå‰è¨˜ç²¾ç¥éšœå®³ã§ã‚ã‚‹ã‹å¦ã‹ã‚’åˆ†æã™ã‚‹åˆ†æéƒ¨ã‚’å‚™ãˆã‚‹ã€ ç²¾ç¥éšœå®³åˆ†...\n",
            "\n",
            "âœ… ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†ï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æº–å‚™ã¨å®Ÿè¡Œ"
      ],
      "metadata": {
        "id": "d9pqdb1TL82h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.training_utils import TrainingManager\n",
        "import time\n",
        "\n",
        "print(\"ğŸš€ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  ğŸ“„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ç‰¹è¨±æ–‡æ›¸ ({len(dataset):,}ä»¶)\")\n",
        "print(f\"  ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {config.model.name}\")\n",
        "print(f\"  ğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—æ•°: {config.training.max_steps}\")\n",
        "print(f\"  âš¡ ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"  ğŸ¯ å­¦ç¿’ç‡: {config.training.learning_rate}\")\n",
        "print(f\"  ğŸ”— LoRAãƒ©ãƒ³ã‚¯: {config.lora.r}\")\n",
        "print(f\"  ğŸ“ æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {config.model.max_seq_length}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
        "training_manager = TrainingManager(config)\n",
        "\n",
        "# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ\n",
        "trainer = training_manager.create_trainer(model, tokenizer, dataset)\n",
        "\n",
        "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²\n",
        "start_time = time.time()\n",
        "\n",
        "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
        "print(\"\\nç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "training_stats = training_manager.train()\n",
        "\n",
        "# å®Ÿè¡Œæ™‚é–“ã®è¨ˆç®—\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nâœ… ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!\")\n",
        "print(f\"å®Ÿè¡Œæ™‚é–“: {training_time/60:.1f}åˆ†\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "3Ae-xlPFW5Y7",
        "outputId": "9d26b29b-cc47-404a-d404-9bff4ced7c95"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:src.training_utils:âœ–ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼: Unsloth: You must specify a `formatting_func`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\n",
            "============================================================\n",
            "  ğŸ“„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ç‰¹è¨±æ–‡æ›¸ (420ä»¶)\n",
            "  ğŸ¤– ãƒ¢ãƒ‡ãƒ«: SakanaAI/TinySwallow-1.5B-Instruct\n",
            "  ğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—æ•°: 200\n",
            "  âš¡ ãƒãƒƒãƒã‚µã‚¤ã‚º: 1\n",
            "  ğŸ¯ å­¦ç¿’ç‡: 3e-05\n",
            "  ğŸ”— LoRAãƒ©ãƒ³ã‚¯: 16\n",
            "  ğŸ“ æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 2048\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unsloth: You must specify a `formatting_func`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1531322106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/01tuning/src/training_utils.py\u001b[0m in \u001b[0;36mcreate_trainer\u001b[0;34m(self, model, tokenizer, dataset)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/trainer.py\u001b[0m in \u001b[0;36mnew_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/01tuning/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0mfix_zero_training_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/01tuning/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0;34m\"dataset, or disable `completion_only_loss` in `SFTConfig`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 )\n\u001b[0;32m--> 624\u001b[0;31m             train_dataset = self._prepare_dataset(\n\u001b[0m\u001b[1;32m    625\u001b[0m                 \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatting_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             )\n",
            "\u001b[0;32m/content/01tuning/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m_prepare_dataset\u001b[0;34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0mdo_formatting_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mformatting_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: You must specify a `formatting_func`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: You must specify a `formatting_func`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã‚’ç¢ºèª"
      ],
      "metadata": {
        "id": "VCJUso_eMheJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç¢ºèª\n",
        "\n",
        "summary = training_manager.get_training_summary()\n",
        "final_memory = model_manager.get_memory_stats()\n",
        "\n",
        "print(f\"\\nğŸ“Š ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å­¦ç¿’çµæœ:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"ğŸ“ˆ æœ€çµ‚Loss: {summary['train_loss']:.4f}\")\n",
        "print(f\"â±ï¸ å®Ÿè¡Œæ™‚é–“: {summary['train_runtime_minutes']:.1f}åˆ†\")\n",
        "print(f\"ğŸš€ 1ç§’ã‚ãŸã‚Šã‚µãƒ³ãƒ—ãƒ«æ•°: {summary['train_samples_per_second']:.2f}\")\n",
        "print(f\"ğŸ’¾ æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory['used']:.2f}GB / {final_memory['total']:.2f}GB ({final_memory['percentage']:.1f}%)\")\n",
        "\n",
        "print(\"\\nğŸ‰ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å°‚ç”¨LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆåŠŸï¼\")"
      ],
      "metadata": {
        "id": "ULZ4oxQoMkBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ¨è«–ãƒ†ã‚¹ãƒˆ"
      ],
      "metadata": {
        "id": "SmSx_PDxMunO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.inference_utils import InferenceManager\n",
        "\n",
        "print(\"ğŸ§ª ç‰¹è¨±å®Ÿæ–½å½¢æ…‹ç”Ÿæˆãƒ†ã‚¹ãƒˆé–‹å§‹...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# æ¨è«–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
        "inference_manager = InferenceManager(model, tokenizer)\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«\n",
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"åŒ–å­¦åˆæˆæ–¹æ³•\",\n",
        "        \"claims\": \"ã€è«‹æ±‚é …ï¼‘ã€‘åŒ–åˆç‰©Aã¨åŒ–åˆç‰©Bã¨ã‚’è§¦åª’Cã®å­˜åœ¨ä¸‹ã§åå¿œã•ã›ã‚‹ã“ã¨ã«ã‚ˆã‚ŠåŒ–åˆç‰©Dã‚’è£½é€ ã™ã‚‹æ–¹æ³•ã§ã‚ã£ã¦ã€åå¿œæ¸©åº¦ãŒï¼˜ï¼â„ƒï½ï¼‘ï¼’ï¼â„ƒã§ã‚ã‚‹ã“ã¨ã‚’ç‰¹å¾´ã¨ã™ã‚‹è£½é€ æ–¹æ³•ã€‚\",\n",
        "        \"context\": \"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"åŠå°ä½“ãƒ‡ãƒã‚¤ã‚¹\",\n",
        "        \"claims\": \"ã€è«‹æ±‚é …ï¼‘ã€‘åŸºæ¿ã¨ã€è©²åŸºæ¿ä¸Šã«å½¢æˆã•ã‚ŒãŸç¬¬ï¼‘ã®åŠå°ä½“å±¤ã¨ã€è©²ç¬¬ï¼‘ã®åŠå°ä½“å±¤ä¸Šã«å½¢æˆã•ã‚ŒãŸçµ¶ç¸å±¤ã¨ã€è©²çµ¶ç¸å±¤ä¸Šã«å½¢æˆã•ã‚ŒãŸç¬¬ï¼’ã®åŠå°ä½“å±¤ã¨ã‚’æœ‰ã™ã‚‹åŠå°ä½“è£…ç½®ã€‚\",\n",
        "        \"context\": \"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"é›»æ± ã‚·ã‚¹ãƒ†ãƒ \",\n",
        "        \"claims\": \"ã€è«‹æ±‚é …ï¼‘ã€‘æ­£æ¥µã¨è² æ¥µã¨ã®é–“ã«é›»è§£è³ªå±¤ã‚’æœ‰ã™ã‚‹é›»æ± ã‚»ãƒ«ã¨ã€è©²é›»æ± ã‚»ãƒ«ã®æ¸©åº¦ã‚’æ¤œå‡ºã™ã‚‹æ¸©åº¦ã‚»ãƒ³ã‚µã¨ã€è©²æ¸©åº¦ã‚»ãƒ³ã‚µã®å‡ºåŠ›ã«åŸºã¥ã„ã¦é›»æ± ã‚»ãƒ«ã®å……æ”¾é›»ã‚’åˆ¶å¾¡ã™ã‚‹åˆ¶å¾¡å›è·¯ã¨ã‚’å‚™ãˆã‚‹é›»æ± ã‚·ã‚¹ãƒ†ãƒ ã€‚\",\n",
        "        \"context\": \"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# å„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§å®Ÿæ–½å½¢æ…‹ç”Ÿæˆã‚’ãƒ†ã‚¹ãƒˆ\n",
        "for i, test_case in enumerate(test_cases, 1):\n",
        "    print(f\"\\n ãƒ†ã‚¹ãƒˆ{i}: {test_case['name']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ\n",
        "    instruction = f\"ä»¥ä¸‹ã®ç‰¹è¨±è«‹æ±‚é …ã‹ã‚‰ã€Œç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹ã€ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\\n\\n### ç‰¹è¨±è«‹æ±‚é …:\\n{test_case['claims']}\\n\\n### ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹:\"\n",
        "\n",
        "    # æ¨è«–å®Ÿè¡Œ\n",
        "    response = inference_manager.test_alpaca_format(\n",
        "        instruction=instruction,\n",
        "        input_text=test_case['context']\n",
        "    )\n",
        "\n",
        "    # çµæœã‹ã‚‰å®Ÿæ–½å½¢æ…‹éƒ¨åˆ†ã‚’æŠ½å‡º\n",
        "    if \"### Response:\" in response:\n",
        "        implementation = response.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        implementation = response.strip()\n",
        "\n",
        "    print(f\" å…¥åŠ›äº‹é …:\")\n",
        "    print(f\" {test_case['claims'][:100]}...\")\n",
        "\n",
        "    print(f\"\\n ç”Ÿæˆã•ã‚ŒãŸå®Ÿæ–½å½¢æ…‹:\")\n",
        "    if len(implementation) > 300:\n",
        "       print(f\"  {implementation[:300]}...\")\n",
        "    else:\n",
        "       print(f\"  {implementation}\")\n",
        "\n",
        "    print(f\"\\nğŸ“Š ç”Ÿæˆçµ±è¨ˆ:\")\n",
        "    print(f\"  æ–‡å­—æ•°: {len(implementation):,}æ–‡å­—\")\n",
        "    print(f\"  ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokenizer.encode(implementation)):,}\")\n",
        "\n",
        "print(\"\\nâœ… ç‰¹è¨±å®Ÿæ–½å½¢æ…‹ç”Ÿæˆãƒ†ã‚¹ãƒˆå®Œäº†\")\n",
        "print(\"\\nğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ:\")\n",
        "print(\"â€¢ ç”Ÿæˆã•ã‚ŒãŸå®Ÿæ–½å½¢æ…‹ã¯æŠ€è¡“çš„ã«è©³ç´°ã§å…·ä½“çš„ãªå†…å®¹ã«ãªã£ã¦ã„ã‚‹ã‹\")\n",
        "print(\"â€¢ è«‹æ±‚é …ã®æŠ€è¡“å†…å®¹ã‚’é©åˆ‡ã«å±•é–‹ã§ãã¦ã„ã‚‹ã‹\")\n",
        "print(\"â€¢ ç‰¹è¨±æ–‡æ›¸ã®å½¢å¼ï¼ˆæ®µè½ç•ªå·ç­‰ï¼‰ã«å¾“ã£ã¦ã„ã‚‹ã‹\")"
      ],
      "metadata": {
        "id": "j0cmmrsxOCih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç‰¹è¨±å°‚ç”¨è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹"
      ],
      "metadata": {
        "id": "ssUXVXCYfTBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from rouge_score import rouge_scorer\n",
        "from sacrebleu import sentence_bleu\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "print(\"ğŸ“Š ç‰¹è¨±å°‚ç”¨è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè¡Œä¸­...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def evaluate_patent_implementation_quality(generated_text, reference_text=None):\n",
        "    \"\"\"ç‰¹è¨±å®Ÿæ–½å½¢æ…‹ã®å“è³ªè©•ä¾¡\"\"\"\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    # 1. æ§‹é€ çš„è©•ä¾¡ï¼šæ®µè½ç•ªå·ã®å­˜åœ¨\n",
        "    paragraph_pattern = r'ã€\\d{4}ã€‘'\n",
        "    paragraph_matches = re.findall(paragraph_pattern, generated_text)\n",
        "    scores['paragraph_structure'] = len(paragraph_matches) > 0\n",
        "    scores['paragraph_count'] = len(paragraph_matches)\n",
        "\n",
        "    # 2. æŠ€è¡“çš„è©³ç´°åº¦: æ•°å€¤ã‚„å…·ä½“çš„ãªå€¤ã®å­˜åœ¨\n",
        "    technical_patterns = [\n",
        "        r'\\d+\\.\\d+[Î¼mn]*[mMA]',  # å¯¸æ³•ï¼ˆä¾‹ï¼š1.5Î¼mï¼‰\n",
        "        r'\\d+â„ƒ',                 # æ¸©åº¦ï¼ˆä¾‹ï¼š100â„ƒï¼‰\n",
        "        r'\\d+ï½\\d+',             # ç¯„å›²ï¼ˆä¾‹ï¼š50ï½100ï¼‰\n",
        "        r'ç´„\\d+',                # æ¦‚æ•°ï¼ˆä¾‹ï¼šç´„500ï¼‰\n",
        "    ]\n",
        "\n",
        "    technical_details = 0\n",
        "    for pattern in technical_patterns:\n",
        "        technical_details += len(re.findall(pattern, generated_text))\n",
        "\n",
        "    scores['technical_detail_count'] = technical_details\n",
        "    scores['has_technical_details'] = technical_details > 0\n",
        "\n",
        "    # 3. ç‰¹è¨±ç”¨èªã®é©åˆ‡æ€§\n",
        "    patent_terms = [\n",
        "        'å®Ÿæ–½å½¢æ…‹', 'å¥½ã¾ã—ãã¯', 'ç‰¹å¾´', 'æ§‹æˆ', 'å½¢æˆ',\n",
        "        'å«ã‚€', 'æœ‰ã™ã‚‹', 'è¨­ã‘ã‚‹', 'é…ç½®', 'æ¥ç¶š'\n",
        "    ]\n",
        "\n",
        "    patent_term_count = sum(1 for term in patent_terms if term in generated_text)\n",
        "    scores['patent_terminology'] = patent_term_count / len(patent_terms)\n",
        "\n",
        "    # 4. æ–‡å­—æ•°ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
        "    scores['character_count'] = len(generated_text)\n",
        "    scores['token_count'] = len(tokenizer.encode(generated_text))\n",
        "\n",
        "    # 5. ROUGEè©•ä¾¡ï¼ˆå‚ç…§ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆï¼‰\n",
        "    if reference_text:\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "        rouge_scores = scorer.score(reference_text, generated_text)\n",
        "\n",
        "        scores['rouge1_f'] = rouge_scores['rouge1'].fmeasure\n",
        "        scores['rouge2_f'] = rouge_scores['rouge2'].fmeasure\n",
        "        scores['rougeL_f'] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "JAKRuDY-ZHDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§è©•ä¾¡ã‚’å®Ÿè¡Œ\n",
        "print(\"\\nğŸ” ç”Ÿæˆã•ã‚ŒãŸå®Ÿæ–½å½¢æ…‹ã®å“è³ªè©•ä¾¡:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# å…ˆã»ã©ã®ãƒ†ã‚¹ãƒˆçµæœã‚’ä½¿ç”¨ï¼ˆå®Ÿéš›ã«ã¯æœ€å¾Œã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®çµæœã‚’ä½¿ç”¨ï¼‰\n",
        "test_implementation = \"\"\"ã€ï¼ï¼ï¼ï¼˜ã€‘æœ¬ç™ºæ˜ã®é›»æ± ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦ã€å›³é¢ã‚’å‚ç…§ã—ã¦è©³ç´°ã«èª¬æ˜ã™ã‚‹ã€‚\n",
        "\n",
        "ã€ï¼ï¼ï¼ï¼™ã€‘å›³ï¼‘ã«ç¤ºã™ã‚ˆã†ã«ã€æœ¬å®Ÿæ–½å½¢æ…‹ã®é›»æ± ã‚·ã‚¹ãƒ†ãƒ ï¼‘ï¼ã¯ã€é›»æ± ã‚»ãƒ«ï¼‘ï¼‘ã€æ¸©åº¦ã‚»ãƒ³ã‚µï¼‘ï¼’ã€ãŠã‚ˆã³åˆ¶å¾¡å›è·¯ï¼‘ï¼“ã‚’å‚™ãˆã¦ã„ã‚‹ã€‚é›»æ± ã‚»ãƒ«ï¼‘ï¼‘ã¯ã€æ­£æ¥µï¼‘ï¼‘ï½ã¨è² æ¥µï¼‘ï¼‘ï½‚ã¨ã®é–“ã«é›»è§£è³ªå±¤ï¼‘ï¼‘ï½ƒã‚’æœ‰ã—ã¦ã„ã‚‹ã€‚\n",
        "\n",
        "ã€ï¼ï¼ï¼‘ï¼ã€‘æ¸©åº¦ã‚»ãƒ³ã‚µï¼‘ï¼’ã¯ã€é›»æ± ã‚»ãƒ«ï¼‘ï¼‘ã®è¡¨é¢ã«é…ç½®ã•ã‚Œã¦ãŠã‚Šã€é›»æ± ã‚»ãƒ«ï¼‘ï¼‘ã®æ¸©åº¦ã‚’é€£ç¶šçš„ã«æ¤œå‡ºã™ã‚‹ã€‚æ¸©åº¦ã‚»ãƒ³ã‚µï¼‘ï¼’ã¨ã—ã¦ã¯ã€ã‚µãƒ¼ãƒŸã‚¹ã‚¿ã‚„ï¼²ï¼´ï¼¤ï¼ˆæŠµæŠ—æ¸©åº¦æ¤œå‡ºå™¨ï¼‰ã‚’ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
        "\n",
        "ã€ï¼ï¼ï¼‘ï¼‘ã€‘åˆ¶å¾¡å›è·¯ï¼‘ï¼“ã¯ã€æ¸©åº¦ã‚»ãƒ³ã‚µï¼‘ï¼’ã®å‡ºåŠ›ä¿¡å·ã‚’å—ä¿¡ã—ã€é›»æ± ã‚»ãƒ«ï¼‘ï¼‘ã®æ¸©åº¦ãŒæ‰€å®šã®ç¯„å›²ï¼ˆä¾‹ãˆã°ï¼â„ƒï½ï¼–ï¼â„ƒï¼‰å†…ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚æ¸©åº¦ãŒç¯„å›²å¤–ã®å ´åˆã€åˆ¶å¾¡å›è·¯ï¼‘ï¼“ã¯å……æ”¾é›»ã‚’åœæ­¢ã¾ãŸã¯åˆ¶é™ã™ã‚‹ã€‚\"\"\"\n",
        "\n",
        "# è©•ä¾¡å®Ÿè¡Œ\n",
        "quality_scores = evaluate_patent_implementation_quality(test_implementation)\n",
        "\n",
        "print(f\"ğŸ“‹ æ§‹é€ çš„è©•ä¾¡:\")\n",
        "print(f\"  âœ… æ®µè½æ§‹é€ : {'æœ‰' if quality_scores['paragraph_structure'] else 'ç„¡'}\")\n",
        "print(f\"  ğŸ“„ æ®µè½æ•°: {quality_scores['paragraph_count']}å€‹\")\n",
        "\n",
        "print(f\"\\nğŸ”¬ æŠ€è¡“çš„è©³ç´°åº¦:\")\n",
        "print(f\"  âœ… æŠ€è¡“çš„è©³ç´°: {'æœ‰' if quality_scores['has_technical_details'] else 'ç„¡'}\")\n",
        "print(f\"  ğŸ“Š æŠ€è¡“è©³ç´°æ•°: {quality_scores['technical_detail_count']}å€‹\")\n",
        "\n",
        "print(f\"\\nğŸ“ ç‰¹è¨±ç”¨èªé©åˆ‡æ€§:\")\n",
        "print(f\"  ğŸ“‹ ç”¨èªã‚¹ã‚³ã‚¢: {quality_scores['patent_terminology']:.2f}\")\n",
        "\n",
        "print(f\"\\nğŸ“ æ–‡ç« çµ±è¨ˆ:\")\n",
        "print(f\"  ğŸ“„ æ–‡å­—æ•°: {quality_scores['character_count']:,}æ–‡å­—\")\n",
        "print(f\"  ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {quality_scores['token_count']:,}\")\n",
        "\n",
        "# ç·åˆè©•ä¾¡ã‚¹ã‚³ã‚¢ã®ç®—å‡º\n",
        "overall_score = (\n",
        "    (1.0 if quality_scores['paragraph_structure'] else 0.0) * 0.3 +\n",
        "    (1.0 if quality_scores['has_technical_details'] else 0.0) * 0.3 +\n",
        "    quality_scores['patent_terminology'] * 0.2 +\n",
        "    (1.0 if 200 <= quality_scores['character_count'] <= 2000 else 0.5) * 0.2\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ¯ ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {overall_score:.2f}/1.0\")\n",
        "\n",
        "if overall_score >= 0.8:\n",
        "    print(\"ğŸŒŸ å„ªç§€: é«˜å“è³ªãªç‰¹è¨±å®Ÿæ–½å½¢æ…‹ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™\")\n",
        "elif overall_score >= 0.6:\n",
        "    print(\"ğŸ‘ è‰¯å¥½: é©åˆ‡ãªå“è³ªã®å®Ÿæ–½å½¢æ…‹ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™\")\n",
        "elif overall_score >= 0.4:\n",
        "    print(\"âš ï¸ æ”¹å–„ä½™åœ°: ã„ãã¤ã‹ã®æ”¹å–„ç‚¹ãŒã‚ã‚Šã¾ã™\")\n",
        "else:\n",
        "    print(\"ğŸ”§ è¦æ”¹å–„: ã‚ˆã‚Šå¤šãã®å­¦ç¿’ãŒå¿…è¦ã§ã™\")\n",
        "\n",
        "print(\"\\nâœ… ç‰¹è¨±å°‚ç”¨è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Œäº†\")\n"
      ],
      "metadata": {
        "id": "2TF5q_Bhjy2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç‰¹è¨±å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"
      ],
      "metadata": {
        "id": "GLSA4El_ugez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ’¾ ç‰¹è¨±å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = f\"models/TinySwallow_Patent_LoRA_{timestamp}\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“ ä¿å­˜å…ˆ: {save_path}\")\n",
        "\n",
        "# LoRAãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
        "print(\"ğŸ”§ LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ä¿å­˜ä¸­...\")\n",
        "model_manager.save_model(save_path)\n",
        "\n",
        "# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¸€ç·’ã«ä¿å­˜\n",
        "config.save_yaml(f\"{save_path}/patent_config.yaml\")\n",
        "\n",
        "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµ±è¨ˆã®ä¿å­˜\n",
        "training_summary = training_manager.get_training_summary()\n",
        "training_summary['model_info'] = model_manager.get_model_info()\n",
        "training_summary['evaluation_scores'] = quality_scores\n",
        "\n",
        "import json\n",
        "with open(f\"{save_path}/training_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(training_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ… ç‰¹è¨±å°‚ç”¨LoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†\")\n",
        "print(f\"ğŸ“ ä¿å­˜å…ˆ: {save_path}/\")"
      ],
      "metadata": {
        "id": "nPnaTn0guerU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆèª­ã¿è¾¼ã¿\n",
        "print(\"\\nğŸ”„ ä¿å­˜ã—ãŸç‰¹è¨±ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ...\")\n",
        "\n",
        "# æ–°ã—ã„è¨­å®šã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
        "test_config = Config.load_from_yaml(f\"{save_path}/patent_config.yaml\")\n",
        "test_model_manager = ModelManager(test_config)\n",
        "\n",
        "# ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
        "test_model, test_tokenizer = test_model_manager.load_model()\n",
        "\n",
        "# ç°¡å˜ãªæ¨è«–ãƒ†ã‚¹ãƒˆ\n",
        "test_inference = InferenceManager(test_model, test_tokenizer)\n",
        "\n",
        "test_claims = \"ã€è«‹æ±‚é …ï¼‘ã€‘ç‚­ç´ ææ–™ã¨é‡‘å±ææ–™ã¨ã‚’è¤‡åˆåŒ–ã—ã¦ãªã‚‹è¤‡åˆææ–™ã§ã‚ã£ã¦ã€è©²è¤‡åˆææ–™ã®å¼·åº¦ãŒå¾“æ¥ææ–™ã®ï¼’å€ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç‰¹å¾´ã¨ã™ã‚‹è¤‡åˆææ–™ã€‚\"\n",
        "test_instruction = f\"ä»¥ä¸‹ã®ç‰¹è¨±è«‹æ±‚é …ã‹ã‚‰ã€Œç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹ã€ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\\n\\n### ç‰¹è¨±è«‹æ±‚é …:\\n{test_claims}\\n\\n### ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹:\"\n",
        "\n",
        "test_response = test_inference.test_alpaca_format(\n",
        "    test_instruction,\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "print(\"ğŸ“¤ ç‰¹è¨±ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆçµæœ:\")\n",
        "print(\"-\" * 50)\n",
        "if \"### Response:\" in test_response:\n",
        "    response_part = test_response.split(\"### Response:\")[-1].strip()\n",
        "    print(response_part[:200] + \"...\" if len(response_part) > 200 else response_part)\n",
        "else:\n",
        "    print(test_response[:200] + \"...\" if len(test_response) > 200 else test_response)\n",
        "\n",
        "print(\"\\nâœ… ç‰¹è¨±å°‚ç”¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆæˆåŠŸï¼\")\n"
      ],
      "metadata": {
        "id": "e-dk42Wrj4hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== ã‚»ãƒ«10: å®Œäº†ã¨ã‚µãƒãƒªãƒ¼ =====\n",
        "# ç‰¹è¨±å°‚ç”¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º\n",
        "print(\"\\nğŸ‰ TinySwallow-1.5B ç‰¹è¨±å°‚ç”¨LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†ï¼\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ“‹ ç‰¹è¨±å­¦ç¿’å®Ÿè¡Œã‚µãƒãƒªãƒ¼:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {config.model.name}\")\n",
        "print(f\"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ç‰¹è¨±æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ ({len(dataset):,}ä»¶)\")\n",
        "print(f\"ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°: {config.training.max_steps}ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†\")\n",
        "print(f\"â±ï¸ å®Ÿè¡Œæ™‚é–“: {summary['train_runtime_minutes']:.1f} åˆ†\")\n",
        "print(f\"ğŸ“ˆ æœ€çµ‚Loss: {summary['train_loss']:.4f}\")\n",
        "print(f\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {save_path}/\")\n",
        "print(f\"ğŸ¯ å“è³ªã‚¹ã‚³ã‚¢: {overall_score:.2f}/1.0\")\n",
        "\n",
        "print(f\"\\nğŸ­ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–æ©Ÿèƒ½:\")\n",
        "print(\"âœ… è«‹æ±‚é …â†’å®Ÿæ–½å½¢æ…‹ç”Ÿæˆã‚¿ã‚¹ã‚¯\")\n",
        "print(\"âœ… ç‰¹è¨±æ–‡æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ\")\n",
        "print(\"âœ… æŠ€è¡“çš„è©³ç´°è¨˜è¿°èƒ½åŠ›\")\n",
        "print(\"âœ… æ®µè½æ§‹é€ åŒ–ç”Ÿæˆ\")\n",
        "print(\"âœ… ç‰¹è¨±ç”¨èªé©åˆ‡ä½¿ç”¨\")\n",
        "\n",
        "print(f\"\\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å¿œç”¨ï¼‰:\")\n",
        "print(\"1. ã‚ˆã‚Šå¤šãã®ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã§ã®è¿½åŠ å­¦ç¿’ (max_steps=500+)\")\n",
        "print(\"2. æŠ€è¡“åˆ†é‡åˆ¥ã®å°‚é–€ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆåŒ–å­¦ãƒ»æ©Ÿæ¢°ãƒ»é›»æ°—ç­‰ï¼‰\")\n",
        "print(\"3. ç‰¹è¨±ã‚¯ãƒ¬ãƒ¼ãƒ åˆ†ææ©Ÿèƒ½ã®è¿½åŠ \")\n",
        "print(\"4. å…ˆè¡ŒæŠ€è¡“èª¿æŸ»æ”¯æ´æ©Ÿèƒ½ã®é–‹ç™º\")\n",
        "print(\"5. ç‰¹è¨±æ–‡æ›¸è‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰\")\n",
        "\n",
        "print(f\"\\nğŸ“š ç‰¹è¨±AIé–¢é€£ãƒªã‚½ãƒ¼ã‚¹:\")\n",
        "print(\"- TinySwallow: https://huggingface.co/SakanaAI/TinySwallow-1.5B-instruct\")\n",
        "print(\"- ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿å‡¦ç†: 01tuning/src/patent_processing/\")\n",
        "print(\"- ç‰¹è¨±è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: ROUGE + ç‰¹è¨±å°‚ç”¨æŒ‡æ¨™\")\n",
        "print(\"- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: https://github.com/daisuke00001/01tuning\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ ç‰¹è¨±AIé–‹ç™ºã®ã‚³ãƒ„:\")\n",
        "print(\"- æŠ€è¡“åˆ†é‡ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿åé›†ãŒé‡è¦\")\n",
        "print(\"- ç‰¹è¨±æ–‡æ›¸ã®æ§‹é€ çš„ç‰¹å¾´ã‚’æ´»ç”¨\")\n",
        "print(\"- æ³•çš„è¡¨ç¾ã¨æŠ€è¡“çš„è¡¨ç¾ã®ãƒãƒ©ãƒ³ã‚¹\")\n",
        "print(\"- å…ˆè¡ŒæŠ€è¡“ã¨ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆã®æ˜ç¢ºåŒ–\")\n",
        "\n",
        "print(f\"\\nğŸ† ç‰¹è¨±AIã®å¿œç”¨å¯èƒ½æ€§:\")\n",
        "print(\"ğŸ“‹ ç‰¹è¨±å‡ºé¡˜æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ \")\n",
        "print(\"ğŸ” å…ˆè¡ŒæŠ€è¡“èª¿æŸ»ã®è‡ªå‹•åŒ–\")\n",
        "print(\"ğŸ“Š ç‰¹è¨±ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ\")\n",
        "print(\"âš–ï¸ ç‰¹è¨±ä¾µå®³ãƒªã‚¹ã‚¯è©•ä¾¡\")\n",
        "print(\"ğŸ”¬ æŠ€è¡“å‹•å‘åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\")\n",
        "\n",
        "print(\"\\nğŸ¯ Happy Patent AI Development! ğŸ¯\")\n",
        "print(\"\\nğŸ’¼ ç‰¹è¨±åˆ†é‡ã§ã®AIæ´»ç”¨ã«ã‚ˆã‚Šã€çŸ¥çš„è²¡ç”£æ¥­å‹™ã®åŠ¹ç‡åŒ–ã¨å“è³ªå‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã—ã‚‡ã†ï¼\")"
      ],
      "metadata": {
        "id": "ilExhi7-j5cx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}