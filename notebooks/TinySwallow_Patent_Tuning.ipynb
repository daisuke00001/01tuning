{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkOWLY8G6c+PZECx6ChPbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c00c2c9afb084eb898d26c0246d82176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0959f9a954ff4653a0c9ace0a4db0d70",
              "IPY_MODEL_ef1df50403b84f7b90d103aafaf6466f",
              "IPY_MODEL_177c12de0d6e4a9fa6748251bd92bc4b"
            ],
            "layout": "IPY_MODEL_7e851f2853544fab94c9a79f17d3fe3b"
          }
        },
        "0959f9a954ff4653a0c9ace0a4db0d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9620e9da0ac482689763e4bd896c211",
            "placeholder": "​",
            "style": "IPY_MODEL_6b32fb56154c4233aebe5112917a0129",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2):   0%"
          }
        },
        "ef1df50403b84f7b90d103aafaf6466f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1d83aabb73849cc811232369baf969e",
            "max": 420,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_552811f43896429eb4bfe9e72e8fc697",
            "value": 0
          }
        },
        "177c12de0d6e4a9fa6748251bd92bc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b02837fadf684f06845d9e3f9d91d65f",
            "placeholder": "​",
            "style": "IPY_MODEL_73bd297058244d2f8cd6e3f294514188",
            "value": " 0/420 [00:03&lt;?, ? examples/s]"
          }
        },
        "7e851f2853544fab94c9a79f17d3fe3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9620e9da0ac482689763e4bd896c211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b32fb56154c4233aebe5112917a0129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1d83aabb73849cc811232369baf969e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "552811f43896429eb4bfe9e72e8fc697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b02837fadf684f06845d9e3f9d91d65f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73bd297058244d2f8cd6e3f294514188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisuke00001/01tuning/blob/main/notebooks/TinySwallow_Patent_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TinySwallow特許データ LoRAファインチューニング専用ノートブック\n"
      ],
      "metadata": {
        "id": "ySSArs5y8K85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 環境とライブラリのセットアップ"
      ],
      "metadata": {
        "id": "hkArpSia-djD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxHWyX1h0Dq9"
      },
      "outputs": [],
      "source": [
        "# 実行環境の確認とgithubリポジトリのクローン\n",
        "import os\n",
        "import subprocess\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 自動リロード設定（初回のみ）\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# 2. 更新があるたびに実行\n",
        "!git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjTKHUaO_Wz",
        "outputId": "f89dd60e-5c28-4200-d8d8-888ec039cab2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUの確認\n",
        "def setup_patent_environment():\n",
        "    \"\"\"特許データ学収容Google Colab環境をセットアップ\"\"\"\n",
        "    print(\"特許データ専用環境をセットアップ中...\")\n",
        "\n",
        "    # GPU確認\n",
        "    if not os.path.exists('/opt/bin/nvidia-smi'):\n",
        "       print(\"GPU環境が検出されません。ランタイムタイプをGPUに変更してください。\")\n",
        "       return False\n",
        "\n",
        "    # リポジトリクローン\n",
        "    repo_url = \"https://github.com/daisuke00001/01tuning.git\"\n",
        "    if os.path.exists(\"01tuning\"):\n",
        "        print(\"📁 リポジトリが既に存在します。最新版に更新中...\")\n",
        "        subprocess.run([\"git\", \"-C\", \"01tuning\", \"pull\"], check=True)\n",
        "    else:\n",
        "        print(f\"📁 リポジトリをクローン中: {repo_url}\")\n",
        "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "    # 作業ディレクトリに移動\n",
        "    os.chdir(\"01tuning\")\n",
        "\n",
        "    # Pythonパスに追加\n",
        "    if \"/content/01tuning/src\" not in sys.path:\n",
        "        sys.path.append(\"/content/01tuning/src\")\n",
        "\n",
        "    print(\"✅ 特許データ学習環境セットアップ完了\")\n",
        "    return True\n",
        "\n",
        "# セットアップ実行\n",
        "setup_patent_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq3IroC01_5B",
        "outputId": "b639f06c-8fbd-4147-e9c2-fbe7c6df4d18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "特許データ専用環境をセットアップ中...\n",
            "📁 リポジトリが既に存在します。最新版に更新中...\n",
            "✅ 特許データ学習環境セットアップ完了\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ライブラリインストール"
      ],
      "metadata": {
        "id": "zqSIn8XmJQUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab専用のライブラリインストール\n",
        "%%capture\n",
        "# Unslothとその依存関係\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# ライブラリエラー対応\n",
        "!pip install xformers\n",
        "!pip uninstall torchvision -y\n",
        "!pip install torchvision --no-cache-dir\n",
        "\n",
        "# 特許データ処理専用ライブラリ\n",
        "!pip install lxml beautifulsoup4 xmltodict\n",
        "!pip install nltk rouge-score sacrebleu\n",
        "!pip install openpyxl\n",
        "\n",
        "# その他の依存関係\n",
        "!pip install PyYAML\n",
        "\n",
        "print(\"📦 特許データ処理用ライブラリインストール完了\")"
      ],
      "metadata": {
        "id": "QK932-njJOEC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 特許専用設定の読み込み"
      ],
      "metadata": {
        "id": "DyOQcXRKJdWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from src.config import Config\n",
        "\n",
        "config_path = \"configs/patent_config.yaml\"\n",
        "config = Config.load_from_yaml(config_path)\n",
        "\n",
        "print(\"⚙️ 特許データ専用設定を読み込み中...\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"📄 設定ファイル: {config_path}\")\n",
        "print(f\"🤖 モデル: {config.model.name}\")\n",
        "print(f\"📊 データセット: {config.dataset.name}\")\n",
        "print(f\"🔧 最大シーケンス長: {config.model.max_seq_length}\")\n",
        "print(f\"📈 最大ステップ数: {config.training.max_steps}\")\n",
        "print(f\"⚡ バッチサイズ: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"🎯 学習率: {config.training.learning_rate}\")\n",
        "print(f\"🔗 LoRAランク: {config.lora.r}\")\n",
        "print(f\"📋 対象モジュール: {', '.join(config.lora.target_modules[:3])}...\")\n",
        "\n",
        "print(\"\\n🏭 特許データ固有設定:\")\n",
        "print(f\"📄 特許文書最大長: {config.dataset.max_patent_length}\")\n",
        "print(f\"📝 実施形態最大長: {config.dataset.max_implementation_length}\")\n",
        "print(f\"🧹 参照除去: {config.dataset.remove_references}\")\n",
        "print(f\"✨ フォーマット整理: {config.dataset.clean_formatting}\")\n",
        "print(f\"📊 評価メトリクス: {', '.join(config.evaluation.metrics)}\")\n",
        "\n",
        "print(\"\\n✅ 特許専用設定読み込み完了\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRjZ43tyJfzV",
        "outputId": "59ea0b5b-126f-45ed-826f-63a617a50f69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ 特許データ専用設定を読み込み中...\n",
            "==================================================\n",
            "📄 設定ファイル: configs/patent_config.yaml\n",
            "🤖 モデル: SakanaAI/TinySwallow-1.5B-Instruct\n",
            "📊 データセット: patent_japanese\n",
            "🔧 最大シーケンス長: 2048\n",
            "📈 最大ステップ数: 200\n",
            "⚡ バッチサイズ: 1\n",
            "🎯 学習率: 3e-05\n",
            "🔗 LoRAランク: 16\n",
            "📋 対象モジュール: q_proj, v_proj, k_proj...\n",
            "\n",
            "🏭 特許データ固有設定:\n",
            "📄 特許文書最大長: 2048\n",
            "📝 実施形態最大長: 1024\n",
            "🧹 参照除去: True\n",
            "✨ フォーマット整理: True\n",
            "📊 評価メトリクス: rouge, bleu, patent_implementation_quality\n",
            "\n",
            "✅ 特許専用設定読み込み完了\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの読み込みとLoRA設定"
      ],
      "metadata": {
        "id": "FV5HnVQqJzYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model_utils import ModelManager\n",
        "import torch\n",
        "\n",
        "print(\"TinySwallow-1.5B モデル読み込み中...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# モデルマネージャークラスの初期化\n",
        "model_manager = ModelManager(config)\n",
        "\n",
        "# メモリ使用量(読み込み前)\n",
        "initial_memory = model_manager.get_memory_stats()\n",
        "print(f\"💾 最終メモリ使用量: {initial_memory['used']:.2f}GB / {initial_memory['total']:.2f}GB ({initial_memory['percentage']:.1f}%)\")\n",
        "\n",
        "# ベースモデルとトークナイザーの読み込み\n",
        "model, tokenizer = model_manager.load_model()\n",
        "\n",
        "# LoRA設定（特許データ専用）\n",
        "print(\"\\n🔧 特許データ専用LoRA設定を適用中...\")\n",
        "model = model_manager.setup_lora()\n",
        "\n",
        "# メモリ使用量（読み込み後）\n",
        "loaded_memory = model_manager.get_memory_stats()\n",
        "print(f\"💾 最終メモリ使用量: {loaded_memory['used']:.2f}GB / {loaded_memory['total']:.2f}GB ({loaded_memory['percentage']:.1f}%)\")\n",
        "\n",
        "# モデル情報の表示\n",
        "model_info = model_manager.get_model_info()  # ✅ 正しいメソッド\n",
        "print(f\"\\n🔍 モデル詳細情報:\")\n",
        "print(f\"📊 総パラメータ数: {model_info['total_params']:,}\")\n",
        "print(f\"🔧 学習可能パラメータ数: {model_info['trainable_params']:,}\")\n",
        "print(f\"📈 学習可能パラメータ割合: {model_info['trainable_percentage']:.2f}%\")\n",
        "\n",
        "print(\"\\n✅ TinySwallow-1.5B + 特許専用LoRA設定完了\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpTe2MjkJ202",
        "outputId": "01c4f559-0b43-4c40-eb99-ad834f49c592"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinySwallow-1.5B モデル読み込み中...\n",
            "==================================================\n",
            "💾 最終メモリ使用量: 0.00GB / 14.74GB (0.0%)\n",
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.7.1+cu126)\n",
            "    Python  3.11.11 (you have 3.11.13)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.11: Fast Qwen2 patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 特許データ専用LoRA設定を適用中...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.7.11 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 最終メモリ使用量: 1.57GB / 14.74GB (10.6%)\n",
            "\n",
            "🔍 モデル詳細情報:\n",
            "📊 総パラメータ数: 907,081,216\n",
            "🔧 学習可能パラメータ数: 18,464,768\n",
            "📈 学習可能パラメータ割合: 2.04%\n",
            "\n",
            "✅ TinySwallow-1.5B + 特許専用LoRA設定完了\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データの準備"
      ],
      "metadata": {
        "id": "2f2WEcQeLAKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"特許データセットを準備中...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Google Drive連携\n",
        "print(\"💾 Google Driveをマウント中...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ Google Driveマウント成功\")\n",
        "\n",
        "# Google Drive内の前処理済みデータパス\n",
        "drive_processed_dir = \"/content/drive/MyDrive/0731PatentData/processed\"\n",
        "local_processed_dir = \"/content/01tuning/data/processed\"\n",
        "\n",
        "# ローカルのprocessedディレクトリを作成\n",
        "os.makedirs(local_processed_dir, exist_ok=True)\n",
        "\n",
        "if os.path.exists(drive_processed_dir):\n",
        "    print(f\"📁 Google Drive内の前処理済みデータを確認中...\")\n",
        "\n",
        "    # 利用可能なファイルをチェック\n",
        "    available_files = []\n",
        "    target_files = [\n",
        "        \"chatml_training.json\",\n",
        "        \"complete_dataset.json\",\n",
        "        \"training_dataset.json\",\n",
        "        \"dataset_stats.json\"\n",
        "    ]\n",
        "\n",
        "    for filename in target_files:\n",
        "        drive_file_path = os.path.join(drive_processed_dir, filename)\n",
        "        if os.path.exists(drive_file_path):\n",
        "            available_files.append(filename)\n",
        "            print(f\"  ✅ {filename} ({os.path.getsize(drive_file_path)/1024/1024:.1f}MB)\")\n",
        "\n",
        "    if available_files:\n",
        "        # ChatML形式データを優先的に使用\n",
        "        if \"chatml_training.json\" in available_files:\n",
        "            target_file = \"chatml_training.json\"\n",
        "        else:\n",
        "            target_file = available_files[0]\n",
        "\n",
        "        print(f\"\\n🔄 {target_file} をローカルにコピー中...\")\n",
        "        drive_file_path = os.path.join(drive_processed_dir, target_file)\n",
        "        local_file_path = os.path.join(local_processed_dir, target_file)\n",
        "\n",
        "        # ファイルをコピー\n",
        "        shutil.copy2(drive_file_path, local_file_path)\n",
        "        print(f\"✅ コピー完了: {local_file_path}\")\n",
        "\n",
        "        # データセットを読み込み（形式に応じて分岐）\n",
        "        print(f\"\\n📂 {target_file} からデータセットを読み込み中...\")\n",
        "\n",
        "        try:\n",
        "            with open(local_file_path, 'r', encoding='utf-8') as f:\n",
        "                # まず全体をJSONとして読み込み\n",
        "                patent_data = json.load(f)\n",
        "\n",
        "                # リストでない場合はリストに変換\n",
        "                if not isinstance(patent_data, list):\n",
        "                    patent_data = [patent_data]\n",
        "\n",
        "                print(f\"📊 読み込み完了（JSON配列形式）:\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # JSON配列形式で失敗した場合、JSONL形式を試行\n",
        "            print(\"🔄 JSONL形式として再試行...\")\n",
        "            with open(local_file_path, 'r', encoding='utf-8') as f:\n",
        "                patent_data = []\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    line = line.strip()\n",
        "                    if line:  # 空行をスキップ\n",
        "                        try:\n",
        "                            patent_data.append(json.loads(line))\n",
        "                        except json.JSONDecodeError as e:\n",
        "                            print(f\"⚠️ 行 {line_num} でJSONエラー: {e}\")\n",
        "                            continue\n",
        "\n",
        "                print(f\"📊 読み込み完了（JSONL形式）:\")\n",
        "\n",
        "        print(f\"  📄 総サンプル数: {len(patent_data):,}\")\n",
        "\n",
        "        if len(patent_data) > 0:\n",
        "            # 最初のサンプルの構造を確認\n",
        "            sample = patent_data[0]\n",
        "            print(f\"  📋 データ構造: {list(sample.keys())}\")\n",
        "\n",
        "            # ChatML形式の場合はテキスト統計を計算\n",
        "            if 'messages' in sample:\n",
        "                print(f\"  🎯 データ形式: ChatML (messages形式)\")\n",
        "                # messagesを単一テキストに変換してテキスト長を計算\n",
        "                total_chars = 0\n",
        "                for item in patent_data[:100]:  # 最初の100件でサンプル計算\n",
        "                    if 'messages' in item:\n",
        "                        text_length = sum(len(msg.get('content', '')) for msg in item['messages'])\n",
        "                        total_chars += text_length\n",
        "                avg_length = total_chars / min(len(patent_data), 100)\n",
        "                print(f\"  📝 平均メッセージ長: {avg_length:.0f} 文字\")\n",
        "\n",
        "            elif 'text' in sample:\n",
        "                print(f\"  🎯 データ形式: 単一テキスト形式\")\n",
        "                avg_length = sum(len(item.get('text', '')) for item in patent_data[:100]) / min(len(patent_data), 100)\n",
        "                print(f\"  📝 平均テキスト長: {avg_length:.0f} 文字\")\n",
        "\n",
        "        # データセット統計情報も読み込み（存在する場合）\n",
        "        stats_file = \"dataset_stats.json\"\n",
        "        if stats_file in available_files:\n",
        "            stats_drive_path = os.path.join(drive_processed_dir, stats_file)\n",
        "            stats_local_path = os.path.join(local_processed_dir, stats_file)\n",
        "            shutil.copy2(stats_drive_path, stats_local_path)\n",
        "\n",
        "            with open(stats_local_path, 'r', encoding='utf-8') as f:\n",
        "                stats = json.load(f)\n",
        "\n",
        "        # 数値フォーマットの安全な適用\n",
        "        total_patents = stats.get('total_patents', 'N/A')\n",
        "        total_claims = stats.get('total_claims', 'N/A')\n",
        "        total_sentences = stats.get('total_sentences', 'N/A')\n",
        "\n",
        "        print(f\"  🏛️ 特許数: {total_patents:,}\" if isinstance(total_patents, (int, float)) else f\"  🏛️ 特許数: {total_patents}\")\n",
        "        print(f\"  📋 請求項数: {total_claims:,}\" if isinstance(total_claims, (int, float)) else f\"  📋 請求項数: {total_claims}\")\n",
        "        print(f\"  📄 総文数: {total_sentences:,}\" if isinstance(total_sentences, (int, float)) else f\"  📄 総文数: {total_sentences}\")\n",
        "\n",
        "        # Hugging Face Dataset形式に変換\n",
        "        dataset = Dataset.from_list(patent_data)\n",
        "        print(f\"\\n🎯 Hugging Face Dataset形式に変換完了\")\n",
        "        print(f\"  Dataset({len(dataset)} rows)\")\n",
        "\n",
        "        # サンプルデータの表示\n",
        "        print(f\"\\n📋 サンプルデータ（最初の1件）:\")\n",
        "        if len(dataset) > 0:\n",
        "            sample = dataset[0]\n",
        "            if 'messages' in sample:\n",
        "                print(f\"  形式: ChatML (messages: {len(sample['messages'])} 件)\")\n",
        "                for i, msg in enumerate(sample['messages'][:2]):  # 最初の2メッセージを表示\n",
        "                    content_preview = msg.get('content', '')[:200] + \"...\" if len(msg.get('content', '')) > 200 else msg.get('content', '')\n",
        "                    print(f\"    {i+1}. {msg.get('role', 'unknown')}: {content_preview}\")\n",
        "            elif 'text' in sample:\n",
        "                text_preview = sample['text'][:300] + \"...\" if len(sample['text']) > 300 else sample['text']\n",
        "                print(f\"  テキスト: {text_preview}\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ 利用可能な前処理済みファイルが見つかりませんでした\")\n",
        "        dataset = None\n",
        "else:\n",
        "    print(f\"❌ Google Drive内に前処理済みデータが見つかりません: {drive_processed_dir}\")\n",
        "    print(\"📁 以下のディレクトリ構造でアップロードしてください:\")\n",
        "    print(\"   Google Drive/PatentData/processed/\")\n",
        "    print(\"     ├── chatml_training.json\")\n",
        "    print(\"     ├── complete_dataset.json\")\n",
        "    print(\"     └── dataset_stats.json\")\n",
        "    dataset = None\n",
        "\n",
        "if dataset is not None:\n",
        "    print(\"\\n✅ 特許データセット準備完了！\")\n",
        "else:\n",
        "    print(\"\\n⚠️ データセットの準備に失敗しました。Google Driveの設定を確認してください。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShdiSWW8LDms",
        "outputId": "0c9d5172-c003-43ce-f22a-c2059d88c983"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "特許データセットを準備中...\n",
            "==================================================\n",
            "💾 Google Driveをマウント中...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Driveマウント成功\n",
            "📁 Google Drive内の前処理済みデータを確認中...\n",
            "  ✅ chatml_training.json (45.1MB)\n",
            "  ✅ complete_dataset.json (123.4MB)\n",
            "  ✅ training_dataset.json (38.9MB)\n",
            "  ✅ dataset_stats.json (0.0MB)\n",
            "\n",
            "🔄 chatml_training.json をローカルにコピー中...\n",
            "✅ コピー完了: /content/01tuning/data/processed/chatml_training.json\n",
            "\n",
            "📂 chatml_training.json からデータセットを読み込み中...\n",
            "📊 読み込み完了（JSON配列形式）:\n",
            "  📄 総サンプル数: 420\n",
            "  📋 データ構造: ['messages', 'metadata']\n",
            "  🎯 データ形式: ChatML (messages形式)\n",
            "  📝 平均メッセージ長: 60278 文字\n",
            "  🏛️ 特許数: N/A\n",
            "  📋 請求項数: N/A\n",
            "  📄 総文数: N/A\n",
            "\n",
            "🎯 Hugging Face Dataset形式に変換完了\n",
            "  Dataset(420 rows)\n",
            "\n",
            "📋 サンプルデータ（最初の1件）:\n",
            "  形式: ChatML (messages: 3 件)\n",
            "    1. system: あなたは特許文書の専門家です。与えられた特許請求の範囲に基づいて、その発明を実施するための具体的な形態を詳しく説明してください。\n",
            "    2. user: 以下の特許請求の範囲に基づいて、発明を実施するための形態を説明してください：\n",
            "\n",
            "【請求項1】\n",
            "精神障害であると診断された患者を含む学習用動画を教師データとして学習させた精神障害学習モデルを記憶させる記憶部と、 分析対象者の日常会話に係る分析対象動画を取得する取得部と、 前記分析対象動画に前記精神障害学習モデルを適用して、前記分析対象者が前記精神障害であるか否かを分析する分析部を備える、 精神障害分...\n",
            "\n",
            "✅ 特許データセット準備完了！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トレーニングの準備と実行"
      ],
      "metadata": {
        "id": "d9pqdb1TL82h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.training_utils import TrainingManager\n",
        "import time\n",
        "\n",
        "print(\"🚀 特許データ専用トレーニング開始...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  📄 データセット: 特許文書 ({len(dataset):,}件)\")\n",
        "print(f\"  🤖 モデル: {config.model.name}\")\n",
        "print(f\"  📈 ステップ数: {config.training.max_steps}\")\n",
        "print(f\"  ⚡ バッチサイズ: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"  🎯 学習率: {config.training.learning_rate}\")\n",
        "print(f\"  🔗 LoRAランク: {config.lora.r}\")\n",
        "print(f\"  📏 最大シーケンス長: {config.model.max_seq_length}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# トレーニングマネージャークラスの初期化\n",
        "training_manager = TrainingManager(config)\n",
        "\n",
        "# トレーナーの作成\n",
        "trainer = training_manager.create_trainer(model, tokenizer, dataset)\n",
        "\n",
        "# トレーニング開始時刻を記録\n",
        "start_time = time.time()\n",
        "\n",
        "# トレーニング実行\n",
        "print(\"\\n特許データ学習を開始します...\")\n",
        "training_stats = training_manager.train()\n",
        "\n",
        "# 実行時間の計算\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"\\n✅ 特許データ専用トレーニング完了!\")\n",
        "print(f\"実行時間: {training_time/60:.1f}分\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c00c2c9afb084eb898d26c0246d82176",
            "0959f9a954ff4653a0c9ace0a4db0d70",
            "ef1df50403b84f7b90d103aafaf6466f",
            "177c12de0d6e4a9fa6748251bd92bc4b",
            "7e851f2853544fab94c9a79f17d3fe3b",
            "a9620e9da0ac482689763e4bd896c211",
            "6b32fb56154c4233aebe5112917a0129",
            "f1d83aabb73849cc811232369baf969e",
            "552811f43896429eb4bfe9e72e8fc697",
            "b02837fadf684f06845d9e3f9d91d65f",
            "73bd297058244d2f8cd6e3f294514188"
          ]
        },
        "id": "3Ae-xlPFW5Y7",
        "outputId": "05f2f665-c925-435b-d5a1-b0b9ef1fa7b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 特許データ専用トレーニング開始...\n",
            "============================================================\n",
            "  📄 データセット: 特許文書 (420件)\n",
            "  🤖 モデル: SakanaAI/TinySwallow-1.5B-Instruct\n",
            "  📈 ステップ数: 200\n",
            "  ⚡ バッチサイズ: 1\n",
            "  🎯 学習率: 3e-05\n",
            "  🔗 LoRAランク: 16\n",
            "  📏 最大シーケンス長: 2048\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/420 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c00c2c9afb084eb898d26c0246d82176"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:src.training_utils:✖トレーナー設定エラー: Column 2 named input_ids expected length 210 but got length 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ArrowInvalid",
          "evalue": "Column 2 named input_ids expected length 210 but got length 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 688, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 3540, in _map_single\n    writer.write_batch(batch, try_original_type=try_original_type)\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\", line 629, in write_batch\n    pa_table = pa.Table.from_arrays(arrays, schema=schema)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/table.pxi\", line 4868, in pyarrow.lib.Table.from_arrays\n  File \"pyarrow/table.pxi\", line 4214, in pyarrow.lib.Table.validate\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Column 2 named input_ids expected length 210 but got length 1\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1531322106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# トレーナーの作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# トレーニング開始時刻を記録\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/01tuning/src/training_utils.py\u001b[0m in \u001b[0;36mcreate_trainer\u001b[0;34m(self, model, tokenizer, dataset)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'messages'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ChatML形式のデータセットを検出、formatting_funcを使用します\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/trainer.py\u001b[0m in \u001b[0;36mnew_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/01tuning/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0mfix_zero_training_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/01tuning/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0;34m\"dataset, or disable `completion_only_loss` in `SFTConfig`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 )\n\u001b[0;32m--> 624\u001b[0;31m             train_dataset = self._prepare_dataset(\n\u001b[0m\u001b[1;32m    625\u001b[0m                 \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatting_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             )\n",
            "\u001b[0;32m/content/01tuning/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m_prepare_dataset\u001b[0;34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_desc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"desc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Unsloth: Tokenizing [\"{dataset_text_field}\"]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmap_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# If VLM, switch data collator since .pad is needed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3169\u001b[0m                         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\" (num_proc={num_proc})\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3170\u001b[0m                     ) as pbar:\n\u001b[0;32m-> 3171\u001b[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001b[0m\u001b[1;32m   3172\u001b[0m                             \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_per_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3173\u001b[0m                         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m_write_generator_to_queue\u001b[0;34m()\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_write_generator_to_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3538\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3539\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3540\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_original_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_original_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3541\u001b[0m                         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4867\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyarrow_wrap_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4868\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4869\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4213\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnogil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4214\u001b[0;31m                 \u001b[0mcheck_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconvert_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: Column 2 named input_ids expected length 210 but got length 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トレーニング結果を確認"
      ],
      "metadata": {
        "id": "VCJUso_eMheJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーニング結果とメモリ使用量を確認\n",
        "\n",
        "summary = training_manager.get_training_summary()\n",
        "final_memory = model_manager.get_memory_stats()\n",
        "\n",
        "print(f\"\\n📊 特許データ学習結果:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"📈 最終Loss: {summary['train_loss']:.4f}\")\n",
        "print(f\"⏱️ 実行時間: {summary['train_runtime_minutes']:.1f}分\")\n",
        "print(f\"🚀 1秒あたりサンプル数: {summary['train_samples_per_second']:.2f}\")\n",
        "print(f\"💾 最終メモリ使用量: {final_memory['used']:.2f}GB / {final_memory['total']:.2f}GB ({final_memory['percentage']:.1f}%)\")\n",
        "\n",
        "print(\"\\n🎉 特許データ専用LoRAチューニング成功！\")"
      ],
      "metadata": {
        "id": "ULZ4oxQoMkBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推論テスト"
      ],
      "metadata": {
        "id": "SmSx_PDxMunO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.inference_utils import InferenceManager\n",
        "\n",
        "print(\"🧪 特許実施形態生成テスト開始...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 推論マネージャークラスの初期化\n",
        "inference_manager = InferenceManager(model, tokenizer)\n",
        "\n",
        "# テスト用サンプル\n",
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"化学合成方法\",\n",
        "        \"claims\": \"【請求項１】化合物Aと化合物Bとを触媒Cの存在下で反応させることにより化合物Dを製造する方法であって、反応温度が８０℃～１２０℃であることを特徴とする製造方法。\",\n",
        "        \"context\": \"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"半導体デバイス\",\n",
        "        \"claims\": \"【請求項１】基板と、該基板上に形成された第１の半導体層と、該第１の半導体層上に形成された絶縁層と、該絶縁層上に形成された第２の半導体層とを有する半導体装置。\",\n",
        "        \"context\": \"\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"電池システム\",\n",
        "        \"claims\": \"【請求項１】正極と負極との間に電解質層を有する電池セルと、該電池セルの温度を検出する温度センサと、該温度センサの出力に基づいて電池セルの充放電を制御する制御回路とを備える電池システム。\",\n",
        "        \"context\": \"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# 各テストケースで実施形態生成をテスト\n",
        "for i, test_case in enumerate(test_cases, 1):\n",
        "    print(f\"\\n テスト{i}: {test_case['name']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # プロンプト作成\n",
        "    instruction = f\"以下の特許請求項から「発明を実施するための形態」を生成してください。\\n\\n### 特許請求項:\\n{test_case['claims']}\\n\\n### 発明を実施するための形態:\"\n",
        "\n",
        "    # 推論実行\n",
        "    response = inference_manager.test_alpaca_format(\n",
        "        instruction=instruction,\n",
        "        input_text=test_case['context']\n",
        "    )\n",
        "\n",
        "    # 結果から実施形態部分を抽出\n",
        "    if \"### Response:\" in response:\n",
        "        implementation = response.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        implementation = response.strip()\n",
        "\n",
        "    print(f\" 入力事項:\")\n",
        "    print(f\" {test_case['claims'][:100]}...\")\n",
        "\n",
        "    print(f\"\\n 生成された実施形態:\")\n",
        "    if len(implementation) > 300:\n",
        "       print(f\"  {implementation[:300]}...\")\n",
        "    else:\n",
        "       print(f\"  {implementation}\")\n",
        "\n",
        "    print(f\"\\n📊 生成統計:\")\n",
        "    print(f\"  文字数: {len(implementation):,}文字\")\n",
        "    print(f\"  トークン数: {len(tokenizer.encode(implementation)):,}\")\n",
        "\n",
        "print(\"\\n✅ 特許実施形態生成テスト完了\")\n",
        "print(\"\\n💡 ポイント:\")\n",
        "print(\"• 生成された実施形態は技術的に詳細で具体的な内容になっているか\")\n",
        "print(\"• 請求項の技術内容を適切に展開できているか\")\n",
        "print(\"• 特許文書の形式（段落番号等）に従っているか\")"
      ],
      "metadata": {
        "id": "j0cmmrsxOCih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 特許専用評価メトリクス"
      ],
      "metadata": {
        "id": "ssUXVXCYfTBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from rouge_score import rouge_scorer\n",
        "from sacrebleu import sentence_bleu\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "print(\"📊 特許専用評価メトリクス実行中...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def evaluate_patent_implementation_quality(generated_text, reference_text=None):\n",
        "    \"\"\"特許実施形態の品質評価\"\"\"\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    # 1. 構造的評価：段落番号の存在\n",
        "    paragraph_pattern = r'【\\d{4}】'\n",
        "    paragraph_matches = re.findall(paragraph_pattern, generated_text)\n",
        "    scores['paragraph_structure'] = len(paragraph_matches) > 0\n",
        "    scores['paragraph_count'] = len(paragraph_matches)\n",
        "\n",
        "    # 2. 技術的詳細度: 数値や具体的な値の存在\n",
        "    technical_patterns = [\n",
        "        r'\\d+\\.\\d+[μmn]*[mMA]',  # 寸法（例：1.5μm）\n",
        "        r'\\d+℃',                 # 温度（例：100℃）\n",
        "        r'\\d+～\\d+',             # 範囲（例：50～100）\n",
        "        r'約\\d+',                # 概数（例：約500）\n",
        "    ]\n",
        "\n",
        "    technical_details = 0\n",
        "    for pattern in technical_patterns:\n",
        "        technical_details += len(re.findall(pattern, generated_text))\n",
        "\n",
        "    scores['technical_detail_count'] = technical_details\n",
        "    scores['has_technical_details'] = technical_details > 0\n",
        "\n",
        "    # 3. 特許用語の適切性\n",
        "    patent_terms = [\n",
        "        '実施形態', '好ましくは', '特徴', '構成', '形成',\n",
        "        '含む', '有する', '設ける', '配置', '接続'\n",
        "    ]\n",
        "\n",
        "    patent_term_count = sum(1 for term in patent_terms if term in generated_text)\n",
        "    scores['patent_terminology'] = patent_term_count / len(patent_terms)\n",
        "\n",
        "    # 4. 文字数・トークン数\n",
        "    scores['character_count'] = len(generated_text)\n",
        "    scores['token_count'] = len(tokenizer.encode(generated_text))\n",
        "\n",
        "    # 5. ROUGE評価（参照テキストがある場合）\n",
        "    if reference_text:\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "        rouge_scores = scorer.score(reference_text, generated_text)\n",
        "\n",
        "        scores['rouge1_f'] = rouge_scores['rouge1'].fmeasure\n",
        "        scores['rouge2_f'] = rouge_scores['rouge2'].fmeasure\n",
        "        scores['rougeL_f'] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "JAKRuDY-ZHDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テストケースで評価を実行\n",
        "print(\"\\n🔍 生成された実施形態の品質評価:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 先ほどのテスト結果を使用（実際には最後のテストケースの結果を使用）\n",
        "test_implementation = \"\"\"【０００８】本発明の電池システムについて、図面を参照して詳細に説明する。\n",
        "\n",
        "【０００９】図１に示すように、本実施形態の電池システム１０は、電池セル１１、温度センサ１２、および制御回路１３を備えている。電池セル１１は、正極１１ａと負極１１ｂとの間に電解質層１１ｃを有している。\n",
        "\n",
        "【００１０】温度センサ１２は、電池セル１１の表面に配置されており、電池セル１１の温度を連続的に検出する。温度センサ１２としては、サーミスタやＲＴＤ（抵抗温度検出器）を用いることができる。\n",
        "\n",
        "【００１１】制御回路１３は、温度センサ１２の出力信号を受信し、電池セル１１の温度が所定の範囲（例えば０℃～６０℃）内にあるかを判定する。温度が範囲外の場合、制御回路１３は充放電を停止または制限する。\"\"\"\n",
        "\n",
        "# 評価実行\n",
        "quality_scores = evaluate_patent_implementation_quality(test_implementation)\n",
        "\n",
        "print(f\"📋 構造的評価:\")\n",
        "print(f\"  ✅ 段落構造: {'有' if quality_scores['paragraph_structure'] else '無'}\")\n",
        "print(f\"  📄 段落数: {quality_scores['paragraph_count']}個\")\n",
        "\n",
        "print(f\"\\n🔬 技術的詳細度:\")\n",
        "print(f\"  ✅ 技術的詳細: {'有' if quality_scores['has_technical_details'] else '無'}\")\n",
        "print(f\"  📊 技術詳細数: {quality_scores['technical_detail_count']}個\")\n",
        "\n",
        "print(f\"\\n📝 特許用語適切性:\")\n",
        "print(f\"  📋 用語スコア: {quality_scores['patent_terminology']:.2f}\")\n",
        "\n",
        "print(f\"\\n📏 文章統計:\")\n",
        "print(f\"  📄 文字数: {quality_scores['character_count']:,}文字\")\n",
        "print(f\"  🔤 トークン数: {quality_scores['token_count']:,}\")\n",
        "\n",
        "# 総合評価スコアの算出\n",
        "overall_score = (\n",
        "    (1.0 if quality_scores['paragraph_structure'] else 0.0) * 0.3 +\n",
        "    (1.0 if quality_scores['has_technical_details'] else 0.0) * 0.3 +\n",
        "    quality_scores['patent_terminology'] * 0.2 +\n",
        "    (1.0 if 200 <= quality_scores['character_count'] <= 2000 else 0.5) * 0.2\n",
        ")\n",
        "\n",
        "print(f\"\\n🎯 総合品質スコア: {overall_score:.2f}/1.0\")\n",
        "\n",
        "if overall_score >= 0.8:\n",
        "    print(\"🌟 優秀: 高品質な特許実施形態が生成されています\")\n",
        "elif overall_score >= 0.6:\n",
        "    print(\"👍 良好: 適切な品質の実施形態が生成されています\")\n",
        "elif overall_score >= 0.4:\n",
        "    print(\"⚠️ 改善余地: いくつかの改善点があります\")\n",
        "else:\n",
        "    print(\"🔧 要改善: より多くの学習が必要です\")\n",
        "\n",
        "print(\"\\n✅ 特許専用評価メトリクス完了\")\n"
      ],
      "metadata": {
        "id": "2TF5q_Bhjy2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 特許専用モデルの保存"
      ],
      "metadata": {
        "id": "GLSA4El_ugez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"💾 特許専用モデルを保存中...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 保存先ディレクトリの作成\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = f\"models/TinySwallow_Patent_LoRA_{timestamp}\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "print(f\"📁 保存先: {save_path}\")\n",
        "\n",
        "# LoRAモデルの保存\n",
        "print(\"🔧 LoRAアダプタを保存中...\")\n",
        "model_manager.save_model(save_path)\n",
        "\n",
        "# 設定ファイルも一緒に保存\n",
        "config.save_yaml(f\"{save_path}/patent_config.yaml\")\n",
        "\n",
        "# トレーニング統計の保存\n",
        "training_summary = training_manager.get_training_summary()\n",
        "training_summary['model_info'] = model_manager.get_model_info()\n",
        "training_summary['evaluation_scores'] = quality_scores\n",
        "\n",
        "import json\n",
        "with open(f\"{save_path}/training_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(training_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ 特許専用LoRAモデル保存完了\")\n",
        "print(f\"📁 保存先: {save_path}/\")"
      ],
      "metadata": {
        "id": "nPnaTn0guerU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存したモデルのテスト読み込み\n",
        "print(\"\\n🔄 保存した特許モデルの読み込みテスト...\")\n",
        "\n",
        "# 新しい設定でモデル読み込み\n",
        "test_config = Config.load_from_yaml(f\"{save_path}/patent_config.yaml\")\n",
        "test_model_manager = ModelManager(test_config)\n",
        "\n",
        "# 保存したモデルを読み込み\n",
        "test_model, test_tokenizer = test_model_manager.load_model()\n",
        "\n",
        "# 簡単な推論テスト\n",
        "test_inference = InferenceManager(test_model, test_tokenizer)\n",
        "\n",
        "test_claims = \"【請求項１】炭素材料と金属材料とを複合化してなる複合材料であって、該複合材料の強度が従来材料の２倍以上であることを特徴とする複合材料。\"\n",
        "test_instruction = f\"以下の特許請求項から「発明を実施するための形態」を生成してください。\\n\\n### 特許請求項:\\n{test_claims}\\n\\n### 発明を実施するための形態:\"\n",
        "\n",
        "test_response = test_inference.test_alpaca_format(\n",
        "    test_instruction,\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "print(\"📤 特許モデル読み込みテスト結果:\")\n",
        "print(\"-\" * 50)\n",
        "if \"### Response:\" in test_response:\n",
        "    response_part = test_response.split(\"### Response:\")[-1].strip()\n",
        "    print(response_part[:200] + \"...\" if len(response_part) > 200 else response_part)\n",
        "else:\n",
        "    print(test_response[:200] + \"...\" if len(test_response) > 200 else test_response)\n",
        "\n",
        "print(\"\\n✅ 特許専用モデル読み込みテスト成功！\")\n"
      ],
      "metadata": {
        "id": "e-dk42Wrj4hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== セル10: 完了とサマリー =====\n",
        "# 特許専用プロジェクト完了のサマリーを表示\n",
        "print(\"\\n🎉 TinySwallow-1.5B 特許専用LoRAチューニングプロジェクト完了！\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"📋 特許学習実行サマリー:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"🤖 モデル: {config.model.name}\")\n",
        "print(f\"📄 データセット: 特許文書データ ({len(dataset):,}件)\")\n",
        "print(f\"🚀 トレーニング: {config.training.max_steps}ステップ完了\")\n",
        "print(f\"⏱️ 実行時間: {summary['train_runtime_minutes']:.1f} 分\")\n",
        "print(f\"📈 最終Loss: {summary['train_loss']:.4f}\")\n",
        "print(f\"💾 モデル保存先: {save_path}/\")\n",
        "print(f\"🎯 品質スコア: {overall_score:.2f}/1.0\")\n",
        "\n",
        "print(f\"\\n🏭 特許データ特化機能:\")\n",
        "print(\"✅ 請求項→実施形態生成タスク\")\n",
        "print(\"✅ 特許文書フォーマット対応\")\n",
        "print(\"✅ 技術的詳細記述能力\")\n",
        "print(\"✅ 段落構造化生成\")\n",
        "print(\"✅ 特許用語適切使用\")\n",
        "\n",
        "print(f\"\\n🚀 次のステップ（特許データ応用）:\")\n",
        "print(\"1. より多くの特許データでの追加学習 (max_steps=500+)\")\n",
        "print(\"2. 技術分野別の専門モデル作成（化学・機械・電気等）\")\n",
        "print(\"3. 特許クレーム分析機能の追加\")\n",
        "print(\"4. 先行技術調査支援機能の開発\")\n",
        "print(\"5. 特許文書自動生成システムの構築\")\n",
        "\n",
        "print(f\"\\n📚 特許AI関連リソース:\")\n",
        "print(\"- TinySwallow: https://huggingface.co/SakanaAI/TinySwallow-1.5B-instruct\")\n",
        "print(\"- 特許データ処理: 01tuning/src/patent_processing/\")\n",
        "print(\"- 特許評価メトリクス: ROUGE + 特許専用指標\")\n",
        "print(\"- プロジェクト: https://github.com/daisuke00001/01tuning\")\n",
        "\n",
        "print(f\"\\n💡 特許AI開発のコツ:\")\n",
        "print(\"- 技術分野ごとのデータ収集が重要\")\n",
        "print(\"- 特許文書の構造的特徴を活用\")\n",
        "print(\"- 法的表現と技術的表現のバランス\")\n",
        "print(\"- 先行技術との差別化ポイントの明確化\")\n",
        "\n",
        "print(f\"\\n🏆 特許AIの応用可能性:\")\n",
        "print(\"📋 特許出願支援システム\")\n",
        "print(\"🔍 先行技術調査の自動化\")\n",
        "print(\"📊 特許ポートフォリオ分析\")\n",
        "print(\"⚖️ 特許侵害リスク評価\")\n",
        "print(\"🔬 技術動向分析レポート\")\n",
        "\n",
        "print(\"\\n🎯 Happy Patent AI Development! 🎯\")\n",
        "print(\"\\n💼 特許分野でのAI活用により、知的財産業務の効率化と品質向上を実現しましょう！\")"
      ],
      "metadata": {
        "id": "ilExhi7-j5cx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}