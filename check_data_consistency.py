#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
from pathlib import Path

def check_data_consistency():
    """ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
    print("=" * 60)
    
    # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    enhanced_path = Path("data/cleaned/enhanced_patents_medium.json")
    if not enhanced_path.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {enhanced_path}")
        return
    
    with open(enhanced_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
    
    # patent_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    patents_by_id = {}
    for item in data:
        patent_id = item.get('patent_id', 'unknown')
        if patent_id not in patents_by_id:
            patents_by_id[patent_id] = {}
        
        section = item.get('section', 'unknown')
        text = item.get('text', '')
        patents_by_id[patent_id][section] = text
    
    print(f"ğŸ“‹ ç‰¹è¨±æ•°: {len(patents_by_id)}")
    
    # å„ç‰¹è¨±ã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
    for patent_id, sections in patents_by_id.items():
        print(f"\nğŸ” ç‰¹è¨±ID: {patent_id}")
        print(f"   ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(sections)}")
        print(f"   åˆ©ç”¨å¯èƒ½ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {list(sections.keys())}")
        
        # claimsã¨detailed_descriptionã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
        claims_text = sections.get('claims', '')
        detailed_desc = sections.get('detailed_description', '')
        
        if claims_text and detailed_desc:
            print(f"\n   ğŸ“œ è«‹æ±‚é … (æœ€åˆã®100æ–‡å­—):")
            print(f"   {claims_text[:100]}...")
            
            print(f"\n   ğŸ“‹ å®Ÿæ–½å½¢æ…‹ (æœ€åˆã®100æ–‡å­—):")
            print(f"   {detailed_desc[:100]}...")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            claims_keywords = extract_keywords(claims_text)
            desc_keywords = extract_keywords(detailed_desc)
            
            common_keywords = claims_keywords & desc_keywords
            print(f"\n   ğŸ”— å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {list(common_keywords)[:5]}")
            
            if len(common_keywords) < 2:
                print(f"   âš ï¸  æ•´åˆæ€§å•é¡Œã®å¯èƒ½æ€§: å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå°‘ãªã„ ({len(common_keywords)}å€‹)")
            else:
                print(f"   âœ… æ•´åˆæ€§OK: ååˆ†ãªå…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ({len(common_keywords)}å€‹)")
    
    # Chat formatãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
    print(f"\n" + "=" * 60)
    print("ğŸ¯ Chat Formatãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
    
    chat_path = Path("data/chat_format/test_chat_enhanced.json")
    if chat_path.exists():
        with open(chat_path, 'r', encoding='utf-8') as f:
            chat_data = json.load(f)
        
        print(f"ğŸ“Š Chat formatãƒ‡ãƒ¼ã‚¿æ•°: {len(chat_data)}")
        
        for i, item in enumerate(chat_data):
            text = item.get('text', '')
            patent_id = item.get('patent_id', 'unknown')
            
            print(f"\nğŸ” Chat Format #{i+1} (ç‰¹è¨±ID: {patent_id}):")
            
            # userã¨assistantã®å†…å®¹ã‚’æŠ½å‡º
            user_content = extract_user_content(text)
            assistant_content = extract_assistant_content(text)
            
            if user_content and assistant_content:
                print(f"   ğŸ‘¤ User (è«‹æ±‚é …): {user_content[:80]}...")
                print(f"   ğŸ¤– Assistant (å®Ÿæ–½å½¢æ…‹): {assistant_content[:80]}...")
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                user_keywords = extract_keywords(user_content)
                assistant_keywords = extract_keywords(assistant_content)
                common = user_keywords & assistant_keywords
                
                print(f"   ğŸ”— å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {list(common)[:3]}")
                
                if len(common) < 2:
                    print(f"   âš ï¸  æ•´åˆæ€§å•é¡Œ: userã¨assistantã®å†…å®¹ãŒä¸€è‡´ã—ãªã„å¯èƒ½æ€§")
                else:
                    print(f"   âœ… æ•´åˆæ€§OK")

def extract_keywords(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    import re
    
    # æ—¥æœ¬èªã®å˜èªã‚’æŠ½å‡ºï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—3æ–‡å­—ä»¥ä¸Šï¼‰
    keywords = set()
    
    # æ¼¢å­—3æ–‡å­—ä»¥ä¸Š
    kanji_words = re.findall(r'[ä¸€-é¾¯]{3,}', text)
    keywords.update(kanji_words)
    
    # ã‚«ã‚¿ã‚«ãƒŠ3æ–‡å­—ä»¥ä¸Š
    katakana_words = re.findall(r'[ã‚¢-ãƒ³]{3,}', text)
    keywords.update(katakana_words)
    
    # ã€ã€‘å†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    bracket_words = re.findall(r'ã€([^ã€‘]+)ã€‘', text)
    keywords.update(bracket_words)
    
    return keywords

def extract_user_content(chat_text):
    """Chat textã‹ã‚‰useréƒ¨åˆ†ã‚’æŠ½å‡º"""
    import re
    match = re.search(r'<\|im_start\|>user\n(.*?)<\|im_end\|>', chat_text, re.DOTALL)
    return match.group(1).strip() if match else ""

def extract_assistant_content(chat_text):
    """Chat textã‹ã‚‰assistantéƒ¨åˆ†ã‚’æŠ½å‡º"""
    import re
    match = re.search(r'<\|im_start\|>assistant\n(.*?)<\|im_end\|>', chat_text, re.DOTALL)
    return match.group(1).strip() if match else ""

if __name__ == "__main__":
    check_data_consistency()