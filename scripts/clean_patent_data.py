#!/usr/bin/env python3
"""
ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œ
"""

import json
import re
import os
import sys
from pathlib import Path
from typing import List, Dict, Any
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PatentDataCleaner:
    """ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / "data" / "processed"
        self.output_dir = self.project_root / "data" / "cleaned"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.output_dir.mkdir(exist_ok=True)
        
    def clean_patent_text(self, text: str) -> str:
        """ç‰¹è¨±ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not isinstance(text, str):
            return ""
        
        logger.debug(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®æ–‡å­—æ•°: {len(text)}")
        
        # ç•°å¸¸ãªæ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
        patterns_to_remove = [
            r'CHEMICAL\d+',     # CHEMICAL6479ç­‰
            r'LEGAL\d+',        # LEGAL170ç­‰  
            r'MIC[A-Z]*',       # MICAç­‰
            r'CH{2,}',          # CHCHCHCHç­‰ï¼ˆ2æ–‡å­—ä»¥ä¸Šã®é€£ç¶šï¼‰
            r'AL\d+',           # AL20ç­‰
            r'LE[A-Z]*',        # LECHEMICALç­‰
            r'ECH[A-Z]*',       # ECHEMICALç­‰
            r'[A-Z]{6,}',       # 6æ–‡å­—ä»¥ä¸Šã®é€£ç¶šå¤§æ–‡å­—
            r'\d{5,}',          # 5æ¡ä»¥ä¸Šã®é€£ç¶šæ•°å­—
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text)
        
        # é€£ç¶šã™ã‚‹åŒã˜æ–‡å­—ã‚’é™¤å»ï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)
        
        # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã«
        text = re.sub(r'\s+', ' ', text)
        
        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        # è¡Œé ­ãƒ»è¡Œæœ«ã®ç©ºç™½ã‚’é™¤å»
        text = text.strip()
        
        logger.debug(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®æ–‡å­—æ•°: {len(text)}")
        return text
        
    def limit_text_length(self, text: str, max_length: int = 1000) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’åˆ¶é™"""
        if len(text) <= max_length:
            return text
            
        # æ–‡ã®åŒºåˆ‡ã‚Šã§åˆ‡ã‚‹
        sentences = text.split('ã€‚')
        result = ""
        
        for sentence in sentences:
            potential_result = result + sentence + 'ã€‚'
            if len(potential_result) <= max_length:
                result = potential_result
            else:
                break
                
        # æ–‡ã§ã®åŒºåˆ‡ã‚Šã§ä½•ã‚‚æ®‹ã‚‰ãªã„å ´åˆã¯ã€å¼·åˆ¶çš„ã«åˆ‡ã‚‹
        if not result:
            result = text[:max_length] + "..."
            
        return result
        
    def process_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """å€‹åˆ¥ã‚¢ã‚¤ãƒ†ãƒ ã®å‡¦ç†"""
        if not isinstance(item, dict):
            return None
            
        cleaned_item = {}
        
        for key, value in item.items():
            if key == 'text' and isinstance(value, str):
                # ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç†
                cleaned_text = self.clean_patent_text(value)
                cleaned_text = self.limit_text_length(cleaned_text, 800)
                cleaned_item[key] = cleaned_text
                
            elif isinstance(value, str):
                # ãã®ä»–ã®æ–‡å­—åˆ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è»½åº¦ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                cleaned_value = self.clean_patent_text(value)
                # é•·ã™ãã‚‹å ´åˆã¯åˆ¶é™
                if len(cleaned_value) > 500:
                    cleaned_value = cleaned_value[:500] + "..."
                cleaned_item[key] = cleaned_value
                
            elif isinstance(value, list):
                # ãƒªã‚¹ãƒˆï¼ˆclaimsç­‰ï¼‰ã®å‡¦ç†
                cleaned_list = []
                for list_item in value:
                    if isinstance(list_item, str):
                        cleaned_list_item = self.clean_patent_text(list_item)
                        if len(cleaned_list_item) > 300:
                            cleaned_list_item = cleaned_list_item[:300] + "..."
                        cleaned_list.append(cleaned_list_item)
                    else:
                        cleaned_list.append(list_item)
                cleaned_item[key] = cleaned_list
                
            else:
                # ãã®ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãã®ã¾ã¾
                cleaned_item[key] = value
        
        # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        text_field = cleaned_item.get('text', '')
        if len(text_field) < 50:  # æœ€å°é•·ãƒã‚§ãƒƒã‚¯
            return None
            
        return cleaned_item
        
    def load_and_clean_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file_path}")
        
        try:
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
            encodings = ['utf-8', 'utf-8-sig', 'cp932', 'shift_jis']
            data = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        data = json.load(f)
                    logger.info(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {encoding}")
                    break
                except (UnicodeDecodeError, json.JSONDecodeError):
                    continue
            
            if data is None:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {file_path}")
                return []
            
            # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®æ­£è¦åŒ–
            if not isinstance(data, list):
                data = [data]
                
            logger.info(f"èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
            
            # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
            cleaned_data = []
            for i, item in enumerate(data):
                try:
                    cleaned_item = self.process_item(item)
                    if cleaned_item:
                        cleaned_data.append(cleaned_item)
                    
                    if (i + 1) % 100 == 0:
                        logger.info(f"å‡¦ç†é€²æ—: {i + 1}/{len(data)}")
                        
                except Exception as e:
                    logger.warning(f"ã‚¢ã‚¤ãƒ†ãƒ  {i} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {len(cleaned_data)}/{len(data)} ä»¶ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿")
            return cleaned_data
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def save_cleaned_data(self, data: List[Dict[str, Any]], output_filename: str):
        """ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        output_path = self.output_dir / output_filename
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¿å­˜å®Œäº†: {output_path}")
            logger.info(f"ä¿å­˜ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¡¨ç¤º
            file_size = output_path.stat().st_size / (1024 * 1024)
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.2f} MB")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_statistics(self, original_data: List[Dict], cleaned_data: List[Dict]) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ"""
        stats = {
            "processing_summary": {
                "original_count": len(original_data),
                "cleaned_count": len(cleaned_data),
                "retention_rate": len(cleaned_data) / len(original_data) * 100 if original_data else 0
            },
            "text_length_stats": {
                "original_avg_length": 0,
                "cleaned_avg_length": 0,
                "max_length": 0,
                "min_length": float('inf')
            }
        }
        
        # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ
        if original_data:
            original_lengths = [len(str(item.get('text', ''))) for item in original_data]
            stats["text_length_stats"]["original_avg_length"] = sum(original_lengths) / len(original_lengths)
        
        if cleaned_data:
            cleaned_lengths = [len(str(item.get('text', ''))) for item in cleaned_data]
            stats["text_length_stats"]["cleaned_avg_length"] = sum(cleaned_lengths) / len(cleaned_lengths)
            stats["text_length_stats"]["max_length"] = max(cleaned_lengths)
            stats["text_length_stats"]["min_length"] = min(cleaned_lengths)
        
        return stats
    
    def run_cleaning(self):
        """ãƒ¡ã‚¤ãƒ³ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†"""
        logger.info("=" * 60)
        logger.info("ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        logger.info("=" * 60)
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        json_files = list(self.data_dir.glob("*.json"))
        
        if not json_files:
            logger.error(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_dir}")
            return
        
        logger.info(f"è¦‹ã¤ã‹ã£ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«: {len(json_files)}")
        for file_path in json_files:
            logger.info(f"  - {file_path.name}")
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        all_cleaned_data = []
        all_stats = {}
        
        for file_path in json_files:
            # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼ˆçµ±è¨ˆç”¨ï¼‰
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    original_data = json.load(f)
                if not isinstance(original_data, list):
                    original_data = [original_data]
            except:
                original_data = []
            
            # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
            cleaned_data = self.load_and_clean_file(file_path)
            
            if cleaned_data:
                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ä¿å­˜
                output_filename = f"cleaned_{file_path.name}"
                self.save_cleaned_data(cleaned_data, output_filename)
                
                # çµ±è¨ˆç”Ÿæˆ
                stats = self.generate_statistics(original_data, cleaned_data)
                all_stats[file_path.name] = stats
                
                # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                all_cleaned_data.extend(cleaned_data)
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜
        if all_cleaned_data:
            self.save_cleaned_data(all_cleaned_data, "cleaned_all_patents.json")
            
            # å­¦ç¿’ç”¨å°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆæœ€åˆã®50ä»¶ï¼‰
            small_dataset = all_cleaned_data[:50]
            self.save_cleaned_data(small_dataset, "cleaned_patents_small.json")
            
            # ä¸­ã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ€åˆã®200ä»¶ï¼‰
            medium_dataset = all_cleaned_data[:200]
            self.save_cleaned_data(medium_dataset, "cleaned_patents_medium.json")
        
        # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
        stats_path = self.output_dir / "cleaning_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(all_stats, f, ensure_ascii=False, indent=2)
        
        logger.info("=" * 60)
        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
        logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿æ•°: {len(all_cleaned_data)}")
        logger.info("=" * 60)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        if all_cleaned_data:
            sample = all_cleaned_data[0]
            logger.info("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
            logger.info(f"  ã‚­ãƒ¼: {list(sample.keys())}")
            if 'text' in sample:
                logger.info(f"  ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(sample['text'])}")
                logger.info(f"  ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {sample['text'][:150]}...")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    cleaner = PatentDataCleaner()
    cleaner.run_cleaning()

if __name__ == "__main__":
    main()