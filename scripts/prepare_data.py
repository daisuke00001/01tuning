"""
ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å„ç¨®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€å‰å‡¦ç†ã€ã‚µãƒ³ãƒ—ãƒ«ä½œæˆã‚’è¡Œã†    
"""

import os
import json
import yaml
import shutil
from pathlib import Path
from typing import List, Dict, Any,Optional
import logging
from datasets import load_dataset, Dataset
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataPreparer:
    """ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path or "configs/tinyswallow_config.yaml"
        self.config = self._load_config()
        self.data_dir = Path("data")
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"
        self.samples_dir = self.data_dir / "samples"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self._create_directories()
        
    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
            return self._get_default_config()
        
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’è¿”ã™"""
        return {
            'data':{
            'dataset_name': 'tinyswallow',
            'train_split': 'train',
            'text_field': 'text',
            'dataset_num_proc': 2
            }
        }
        
    def _create_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        for directory in [self.data_dir, self.raw_dir, self.processed_dir, self.samples_dir]:
            directory.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {directory}")
            
    def download_alpaca_dataset(self) -> bool:
        """Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            dataset_name = self.config['data']['dataset_name']
            dataset = load_dataset(dataset_name, split=self.config['data']['train_split'])
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            output_path = self.raw_dir / "alpaca_dataset.json"
            dataset.to_json(output_path)
            
            logger.info(f"âœ… Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜: {output_path}")
            logger.info(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset):,}ä»¶")
            
            return True
        except Exception as e:
            logger.error(f"âŒ Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
            return False
        
    def create_sample_dataset(self) -> bool:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
        
        success = True
        
        # åŸºæœ¬çš„ãªAlpacaã‚µãƒ³ãƒ—ãƒ«
        if not self._create_basic_alpaca_samples():
            success = False
            
        # ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«
        if not self._create_patent_samples():
            success = False
            
        # ã‚«ã‚¹ã‚¿ãƒ ã‚µãƒ³ãƒ—ãƒ«
        if not self._create_custom_samples():
            success = False
            
        return success
    
    def _create_basic_alpaca_samples(self) -> bool:
        """Alpacaã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ"""
        try:
            sample_data = [
                {
                    "instruction": "æ¬¡ã®æ•°åˆ—ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚",
                    "input": "2, 4, 6, 8",
                    "output": "10, 12, 14, 16ã§ã™ã€‚ã“ã‚Œã¯å¶æ•°ã®æ•°åˆ—ã§ã€å„é …ã¯å‰ã®é …ã«2ã‚’åŠ ãˆãŸã‚‚ã®ã§ã™ã€‚"
                },
                {
                    "instruction": "ä»¥ä¸‹ã®æ–‡ç« ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚",
                    "input": "Hello, how are you today?",
                    "output": "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã‹ãŒãŠéã”ã—ã§ã™ã‹ï¼Ÿ"
                },
                {
                    "instruction": "æ¬¡ã®éƒ½å¸‚ã®é¦–éƒ½ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
                    "input": "ãƒ•ãƒ©ãƒ³ã‚¹",
                    "output": "ãƒ•ãƒ©ãƒ³ã‚¹ã®é¦–éƒ½ã¯ãƒ‘ãƒªã§ã™ã€‚"
                },
                {
                    "instruction": "ç°¡å˜ãªPythonã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
                    "input": "1ã‹ã‚‰10ã¾ã§ã®æ•°å­—ã‚’å‡ºåŠ›ã™ã‚‹",
                    "output": "```python\nfor i in range(1, 11):\n    print(i)\n```"
                },
                {
                    "instruction": "æ¬¡ã®æ•°å­¦å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚",
                    "input": "x + 5 = 12ã®xã‚’æ±‚ã‚ã‚ˆ",
                    "output": "x = 12 - 5 = 7ã§ã™ã€‚"
                }           
            ]
            
            output_path = self.samples_dir / "alpaca_samples.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(sample_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… Alpacaã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜: {output_path}")
                return True
            
        except Exception as e:
            logger.error(f"âŒ Alpacaã‚µãƒ³ãƒ—ãƒ«ã®ä½œæˆã«å¤±æ•—: {e}")
            return False
        
    def _create_patent_samples(self) -> bool:
        """ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ"""
        try:
            patent_data = [
                {
                    "instruction": "åŒ–å­¦ç‰¹è¨±ã®ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹ã®æ®µè½ã€0012ã€‘ã‚’ä½œæˆã—ã¦ãã ã•ã„",
                    "input": """ã€è«‹æ±‚é …1ã€‘
                    ä¸‹è¨˜ä¸€èˆ¬å¼ï¼ˆï¼‘ï¼‰ã§è¡¨ã•ã‚Œã‚‹åŒ–åˆç‰©ã€‚
                    ã€åŒ–ï¼‘ã€‘000001
                    ï¼»ä¸€èˆ¬å¼ï¼ˆï¼‘ï¼‰ã«ãŠã„ã¦ã€ï¼¸ï¼‘ãŠã‚ˆã³ï¼¸ï¼’ã¯ã€ä¸€æ–¹ãŒï¼®ã‚’è¡¨ã—ã€ä»–æ–¹ãŒï¼£ï¼ï¼¹ï¼“ã‚’è¡¨ã™ã€‚ï¼¹ï¼‘ï½ï¼¹ï¼“ã¯ï¼‘å€‹ãŒã‚·ã‚¢ãƒåŸºã‚’è¡¨ã—ã€æ®‹ã‚Šã®ï¼’å€‹ãŒå„ã€…ç‹¬ç«‹ã«ç½®æ›ã‚‚ã—ãã¯ç„¡ç½®æ›ã®ã‚¢ãƒªãƒ¼ãƒ«åŸºã€ã¾ãŸã¯ç½®æ›ã‚‚ã—ãã¯ç„¡ç½®æ›ã®ãƒ˜ãƒ†ãƒ­ã‚¢ãƒªãƒ¼ãƒ«åŸºã‚’è¡¨ã™ã€‚ï¼½""",
                    "output": """ã€0012ã€‘æœ¬ç™ºæ˜ã®ä¸€èˆ¬å¼ï¼ˆï¼‘ï¼‰ã§è¡¨ã•ã‚Œã‚‹åŒ–åˆç‰©ã®å…·ä½“çš„ãªåˆæˆä¾‹ã«ã¤ã„ã¦èª¬æ˜ã™ã‚‹ã€‚ã¾ãšã€å¸‚è²©ã®ãƒ™ãƒ³ã‚¾ãƒ‹ãƒˆãƒªãƒ«ï¼ˆï¼‘ï½‡ã€9.7ï½ï½ï½ï½Œï¼‰ã‚’ã‚¸ãƒ¡ãƒãƒ«ãƒ›ãƒ«ãƒ ã‚¢ãƒŸãƒ‰ï¼ˆï¼¤ï¼­ï¼¦ï¼‰20ï½ï¼¬ã«æº¶è§£ã—ã€ç‚­é…¸ã‚«ãƒªã‚¦ãƒ ï¼ˆï¼’ï½‡ã€14.5ï½ï½ï½ï½Œï¼‰ã‚’åŠ ãˆã¦80â„ƒã§ï¼’æ™‚é–“æ’¹æ‹Œã™ã‚‹ã€‚ã“ã®åå¿œã«ã‚ˆã‚Šã€ä¸€èˆ¬å¼ï¼ˆï¼‘ï¼‰ã«ãŠã„ã¦ï¼¹ï¼‘ãŒã‚·ã‚¢ãƒåŸºã€ï¼¹ï¼’ãŠã‚ˆã³ï¼¹ï¼“ãŒãƒ•ã‚§ãƒ‹ãƒ«åŸºã§ã‚ã‚‹åŒ–åˆç‰©ã‚’åç‡78ï¼…ã§å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ç”Ÿæˆç‰©ã¯ç™½è‰²çµæ™¶ã¨ã—ã¦å˜é›¢ã•ã‚Œã€èç‚¹ã¯156-158â„ƒã§ã‚ã‚‹ã€‚"""
                },
                {
                    "instruction": "æ©Ÿæ¢°ç‰¹è¨±ã®å®Ÿæ–½å½¢æ…‹æ®µè½ã€0015ã€‘ã‚’ä½œæˆã—ã¦ãã ã•ã„",
                    "input": """ã€è«‹æ±‚é …1ã€‘
                    åŸºæ¿ã¨ã€å‰è¨˜åŸºæ¿ä¸Šã«è¨­ã‘ã‚‰ã‚ŒãŸå›è»¢è»¸ã¨ã€å‰è¨˜å›è»¢è»¸ã«å–ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸå›è»¢ä½“ã¨ã‚’å‚™ãˆãŸå›è»¢è£…ç½®ã€‚""",
                    "output": """ã€0015ã€‘å›³ï¼‘ã«ç¤ºã™ã‚ˆã†ã«ã€æœ¬å®Ÿæ–½å½¢æ…‹ã®å›è»¢è£…ç½®10ã¯ã€åŸºæ¿11ã€å›è»¢è»¸12ã€ãŠã‚ˆã³å›è»¢ä½“13ã‚’ä¸»è¦ãªæ§‹æˆè¦ç´ ã¨ã—ã¦å‚™ãˆã¦ã„ã‚‹ã€‚åŸºæ¿11ã¯ã€ã‚¢ãƒ«ãƒŸãƒ‹ã‚¦ãƒ åˆé‡‘è£½ã®å¹³æ¿çŠ¶éƒ¨æã§ã‚ã‚Šã€åšã•5mmã®æ¿æã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚å›è»¢è»¸12ã¯ã€åŸºæ¿11ã®ä¸­å¤®éƒ¨ã«å‚ç›´ã«å–ã‚Šä»˜ã‘ã‚‰ã‚Œã¦ãŠã‚Šã€ã‚¹ãƒ†ãƒ³ãƒ¬ã‚¹é‹¼ï¼ˆSUS304ï¼‰è£½ã§ç›´å¾„10mmã€é•·ã•50mmã®å††æŸ±çŠ¶ã§ã‚ã‚‹ã€‚å›è»¢ä½“13ã¯ã€å›è»¢è»¸12ã®ä¸Šç«¯ã«å›ºå®šã•ã‚Œã€ç›´å¾„100mmã®å††ç›¤çŠ¶ã«å½¢æˆã•ã‚Œã¦ã„ã‚‹ã€‚"""
                },
                {
                    "instruction": "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç‰¹è¨±ã®å®Ÿæ–½å½¢æ…‹æ®µè½ã€0018ã€‘ã‚’ä½œæˆã—ã¦ãã ã•ã„",
                    "input": """ã€è«‹æ±‚é …1ã€‘
                    ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã™ã‚‹å—ä¿¡éƒ¨ã¨ã€å—ä¿¡ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹å‡¦ç†éƒ¨ã¨ã€å‡¦ç†çµæœã‚’å‡ºåŠ›ã™ã‚‹å‡ºåŠ›éƒ¨ã¨ã‚’å‚™ãˆãŸæƒ…å ±å‡¦ç†è£…ç½®ã€‚""",
                    "output": """ã€0018ã€‘å›³ï¼“ã«ç¤ºã™ã‚ˆã†ã«ã€æœ¬å®Ÿæ–½å½¢æ…‹ã®æƒ…å ±å‡¦ç†è£…ç½®30ã¯ã€å—ä¿¡éƒ¨31ã€å‡¦ç†éƒ¨32ã€ãŠã‚ˆã³å‡ºåŠ›éƒ¨33ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚å—ä¿¡éƒ¨31ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆNICï¼‰ã‚’ä»‹ã—ã¦TCP/IP ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã™ã‚‹ã€‚å—ä¿¡ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã€JSONå½¢å¼ã¾ãŸã¯XMLå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚Šã€æœ€å¤§1MBã¾ã§ã®ã‚µã‚¤ã‚ºã«å¯¾å¿œã—ã¦ã„ã‚‹ã€‚å‡¦ç†éƒ¨32ã¯ã€CPUï¼ˆIntel Core i7-9700Kï¼‰ã¨ãƒ¡ãƒ¢ãƒªï¼ˆDDR4 16GBï¼‰ã‚’ç”¨ã„ã¦ã€å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã®è§£æãŠã‚ˆã³æ‰€å®šã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ãæ¼”ç®—å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
                }
            ]
            
            output_path = self.samples_dir / "patent_samples.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(patent_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜: {output_path}")
                return True
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã®ä½œæˆã«å¤±æ•—: {e}")
            return False
        
    def _create_custom_samples(self) -> bool:
        """ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ"""
        try:
            custom_data = [
                {
                    "instruction": "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚",
                    "input": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰æŠ€è¡“ã¯æ€¥é€Ÿã«ç™ºå±•ã—ã¦ãŠã‚Šã€ç‰¹ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§é©å‘½çš„ãªé€²æ­©ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€ç¿»è¨³ã€è³ªå•å¿œç­”ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãªã©ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§äººé–“ãƒ¬ãƒ™ãƒ«ã¾ãŸã¯ãã‚Œã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã®é«˜ã•ã‚„ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œãªã©ã€è§£æ±ºã™ã¹ãèª²é¡Œã‚‚å¤šãå­˜åœ¨ã—ã¾ã™ã€‚",
                    "output": "AIæŠ€è¡“ã€ç‰¹ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯è‡ªç„¶è¨€èªå‡¦ç†ã§é©å‘½çš„é€²æ­©ã‚’é‚ã’ã€å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã§å„ªç§€ãªæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚„ãƒã‚¤ã‚¢ã‚¹ãªã©ã®èª²é¡ŒãŒæ®‹å­˜ã—ã¦ã„ã¾ã™ã€‚"
                },
                {
                    "instruction": "æ¬¡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‚¾å‘ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚",
                    "input": "å£²ä¸Šãƒ‡ãƒ¼ã‚¿: 1æœˆ:100ä¸‡å††, 2æœˆ:120ä¸‡å††, 3æœˆ:110ä¸‡å††, 4æœˆ:140ä¸‡å††, 5æœˆ:160ä¸‡å††",
                    "output": "å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®åˆ†æçµæœï¼šå…¨ä½“çš„ã«ä¸Šæ˜‡å‚¾å‘ã‚’ç¤ºã—ã¦ãŠã‚Šã€1æœˆã‹ã‚‰5æœˆã«ã‹ã‘ã¦60ä¸‡å††ï¼ˆ60%ï¼‰ã®å¢—åŠ ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚3æœˆã«ä¸€æ™‚çš„ãªæ¸›å°‘ãŒã‚ã‚Šã¾ã—ãŸãŒã€4æœˆä»¥é™ã¯å®‰å®šã—ãŸæˆé•·ã‚’ç¶™ç¶šã—ã¦ã„ã¾ã™ã€‚æœˆå¹³å‡æˆé•·ç‡ã¯ç´„15%ã§ã™ã€‚"
                },
                {
                    "instruction": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚",
                    "input": "ãƒªã‚¹ãƒˆ[1, 2, 3, 4, 5]ã®å„è¦ç´ ã‚’2å€ã«ã—ã¦æ–°ã—ã„ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
                    "output": "```python\n# æ–¹æ³•1: ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã‚’ä½¿ç”¨\noriginal_list = [1, 2, 3, 4, 5]\ndoubled_list = [x * 2 for x in original_list]\nprint(doubled_list)  # [2, 4, 6, 8, 10]\n\n# æ–¹æ³•2: mapé–¢æ•°ã‚’ä½¿ç”¨\ndoubled_list2 = list(map(lambda x: x * 2, original_list))\nprint(doubled_list2)  # [2, 4, 6, 8, 10]\n```"
                }
            ]
            
            output_path = self.samples_dir / "custom_samples.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(custom_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ã‚«ã‚¹ã‚¿ãƒ ã‚µãƒ³ãƒ—ãƒ«ä½œæˆå®Œäº†: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ã‚«ã‚¹ã‚¿ãƒ ã‚µãƒ³ãƒ—ãƒ«ä½œæˆå¤±æ•—: {e}")
            return False
            
    def validate_data_format(self, data_path: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¤œè¨¼"""
        logger.info(f"ğŸ”ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ¤œè¨¼ä¸­: {data_path}")
        
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if not isinstance(data, list):
                logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã¯ãƒªã‚¹ãƒˆå½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
                return False
            
            required_fields = ['instruction', 'input', 'output']
            
            for i, item in enumerate(data[:5]):
                if not isinstance(item, dict):
                    logger.error(f"âŒ ã‚¢ã‚¤ãƒ†ãƒ {i}ã¯è¾æ›¸å½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
                    return False
                
                for field in required_fields:
                    if field not in item:
                        logger.error(f"âŒ ã‚¢ã‚¤ãƒ†ãƒ {i}ã«{field}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                        return False
                    
                    if not isinstance(item[field], str):
                        logger.error(f"âŒ ã‚¢ã‚¤ãƒ†ãƒ {i}ã®{field}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯æ–‡å­—åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
                        return False
                    
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¤œè¨¼å®Œäº†:  {len(data)}ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ")
            return True
        
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¤œè¨¼ã«å¤±æ•—: {e}")
            return False
        
    def create_data_statistics(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’ä½œæˆ"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’ä½œæˆä¸­...")
        
        try:
            statistics = {}
            
            # å„ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆã‚’ä½œæˆ
            sample_files = list(self.samples_dir.glob("*.json"))
            
            for sample_file in sample_files:
                with open(sample_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    stats = {
                        'total_samples': len(data),
                        'avg_instruction_length': sum(len(item['instruction']) for item in data) / len(data),
                        'avg_input_length': sum(len(item['input']) for item in data) / len(data),
                        'avg_output_length': sum(len(item['output']) for item in data) / len(data),
                        'max_instruction_length': max(len(item['instruction']) for item in data),
                        'max_input_length': max(len(item['input']) for item in data),
                        'max_output_length': max(len(item['output']) for item in data)                                    
                    }
                    
                    statistics[sample_file.stem] = stats
                    
            # çµ±è¨ˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            stats_path = self.processed_dir / "data_statistics.json"
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, ensure_ascii=False, indent=2)
                
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤º
            print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            print("-" * 50)
            for dataset_name, stats in statistics.items():
                print(f"\nğŸ“ {dataset_name}:")
                print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats['total_samples']}")
                print(f"  å¹³å‡æŒ‡ç¤ºé•·: {stats['avg_instruction_length']:.1f}æ–‡å­—")
                print(f"  å¹³å‡å…¥åŠ›é•·: {stats['avg_input_length']:.1f}æ–‡å­—")
                print(f"  å¹³å‡å‡ºåŠ›é•·: {stats['avg_output_length']:.1f}æ–‡å­—")
            
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆä½œæˆå®Œäº†: {stats_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆä½œæˆå¤±æ•—: {e}")
            return False
        
    def run_full_preparation(self) -> bool:
        """å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚’å®Ÿè¡Œ"""
        logger.info("ğŸš€ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚’é–‹å§‹...")
        
        success = True
        
        print("ğŸ“‹ å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        print("2. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ")
        print("3. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¤œè¨¼")
        print("4. ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®ä½œæˆ")
        print("-" * 50)
        
        # 1. Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if not self.download_alpaca_dataset():
            success = False
        
        # 2. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        if not self.create_sample_dataset():
            success = False
        
        # 3. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¤œè¨¼
        sample_files = list(self.samples_dir.glob("*.json"))
        for sample_file in sample_files:
            if not self.validate_data_format(str(sample_file)):
                success = False
        
        # 4. ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®ä½œæˆ
        if not self.create_data_statistics():
            success = False
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("\n" + "="*50)
        if success:
            print("ğŸ‰ ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ï¼")
            print(f"ğŸ“ ç”Ÿãƒ‡ãƒ¼ã‚¿: {self.raw_dir}")
            print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: {self.samples_dir}")
            print(f"ğŸ“ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿: {self.processed_dir}")
        else:
            print("âš ï¸  ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        print("="*50)
        
        return success

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ“Š TinySwallow LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° - ãƒ‡ãƒ¼ã‚¿æº–å‚™")
    print("="*60)
    
    preparer = DataPreparer()
    success = preparer.run_full_preparation()
    
    if not success:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        exit(1)
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Jupyter Notebookã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()