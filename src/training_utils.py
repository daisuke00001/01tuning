# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢é€£ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

from trl import SFTConfig, SFTTrainer
import logging
from config import Config

logger = logging.getLogger(__name__)

class TrainingManager:
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Config):
        self.config = config
        self.model = None
        self.training_stats = None
        
    def create_trainer(self, model, tokenizer, dataset) -> SFTTrainer:
        """ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆ"""
        try:
            logger.info("ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’è¨­å®šä¸­...")

            # SFTConfig è¨­å®šï¼ˆå‹å¤‰æ›ã‚’ç¢ºå®Ÿã«ã™ã‚‹ï¼‰
            sft_config = SFTConfig(
                per_device_train_batch_size=int(self.config.training.per_device_train_batch_size),
                gradient_accumulation_steps=int(self.config.training.gradient_accumulation_steps),
                warmup_steps=int(self.config.training.warmup_steps),
                max_steps=int(self.config.training.max_steps),
                learning_rate=float(self.config.training.learning_rate),
                logging_steps=int(self.config.training.logging_steps),
                optim=str(self.config.training.optim),
                weight_decay=float(self.config.training.weight_decay),
                lr_scheduler_type=str(self.config.training.lr_scheduler_type),
                seed=int(self.config.training.seed),
                output_dir=str(self.config.training.output_dir),
                report_to=str(self.config.training.report_to),
            )

            # save_steps ã¨ save_total_limitãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è¿½åŠ ï¼ˆå‹å¤‰æ›ä»˜ãï¼‰
            if self.config.training.save_steps is not None:
                sft_config.save_steps = int(self.config.training.save_steps)
            if self.config.training.save_total_limit is not None:
                sft_config.save_total_limit = int(self.config.training.save_total_limit)

            # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ
            trainer = SFTTrainer(
                model=model,
                tokenizer=tokenizer,
                train_dataset=dataset,
                dataset_text_field=self.config.data.text_field,
                max_seq_length=self.config.model.max_seq_length,
                dataset_num_proc=self.config.data.dataset_num_proc,
                packing=self.config.data.packing,
                args=sft_config,
            )

            self.trainer = trainer
            logger.info("âœ…ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆå®Œäº†")
            return trainer
        
        except Exception as e:
            logger.error(f"âœ–ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def train(self) -> dict:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        if self.trainer is None:
            raise ValueError("ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚create_trainer()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        try:
            logger.info("ğŸš€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹...")

            self.training_stats = self.trainer.train()
            logger.info("âœ…ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
            return self.training_stats
        
        except Exception as e:
            logger.error(f"âœ–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def get_training_summary(self) -> dict:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if self.training_stats is None:
            return {"error": "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        runtime = self.training_stats.metrics.get('train_runtime', 0)

        return {
            "train_runtime_seconds": round(runtime, 1),
            "train_runtime_minutes": round(runtime / 60, 2),
            "train_samples_per_second": self.training_stats.metrics.get('train_samples_per_second', 0),
            "train_steps_per_second": self.training_stats.metrics.get('train_steps_per_second', 0),
            "total_flos": self.training_stats.metrics.get('total_flos', 0),
            "train_loss": self.training_stats.metrics.get('train_loss', 0),
        }