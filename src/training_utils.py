# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢é€£ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

from trl import SFTConfig, SFTTrainer
import logging

logger = logging.getLogger(__name__)

class TrainingManager:
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Config):
        self.config = config
        self.model = None
        self.tarining_stats = None
        
    def create_trainer(self, model, tokenizer, dataset) -> SFTTrainer:
        """ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆ"""
        try:
            logger.info("ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’è¨­å®šä¸­...")

            # SEFTConfig è¨­å®š
            sft_config = SFTConfig(
                per_device_train_batch_size=self.config.training.per_device_train_batch_size,
                gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,
                warmup_steps=self.config.training.warmup_steps,
                max_steps=self.config.training.max_steps,
                learning_rate=self.config.training.learning_rate,
                logging_steps=self.config.training.logging_steps,
                optim=self.config.training.optim,
                weight_decay=self.config.training.weight_decay,
                lr_scheduler_type=self.config.training.lr_scheduler_type,
                seed=self.config.training.seed,
                output_dir=self.config.training.output_dir,
                report_to=self.config.training.report_to,
            )

            # save_steps ã¨ save_total_limitãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è¿½åŠ 
            if self.config.training.save_steps is not None:
                sft_config.save_steps = self.config.training.save_steps
            if self.config.training.save_total_limit is not None:
                sft_config.save_total_limit = self.config.training.save_total_limit

            # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ
            trainer = SFTTrainer(
                model=model,
                tokenizer=tokenizer,
                train_dataset=dataset,
                deataset_text_field=self.config.data.text_field,
                max_seq_length=self.config.model.max_seq_length,
                dataset_num_proc=self.config.data.dataset_num_proc,
                packing=self.config.training.packing,
                args=sft_config,
            )

            self.trainer = trainer
            logger.info("âœ…ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆå®Œäº†")
            return trainer
        
        except Exception as e:
            logger.error(f"âœ–ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def train(self) -> dict:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        if self.trainer is None:
            raise ValueError("ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚create_trainer()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        try:
            logger.info("ğŸš€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹...")

            self.training_stats = self.trainer.train()
            logger.info("âœ…ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
            return self.training_stats
        
        except Exception as e:
            logger.error(f"âœ–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def get_training_summary(self) -> dict:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if self.training_stats is None:
            return {"error": "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        runtime = self.training_stats.metrics.get('train_runtime', 0)

        return {
            "train_runtime_seconds": round(runtime, 1),
            "train_runtime_minutes": round(runtime / 60, 2),
            "train_samples_per_second": self.training_stats.metrics.get('train_samples_persecond', 0),
            "train_steps_oer_second": self.training_stats.metrics.get('train_steps_persecond', 0),
            "total_flos": self.training_stats.metrics.get('total_flos', 0),
            "train_loss": self.training_stats.metrics.get('train_loss', 0),
        }